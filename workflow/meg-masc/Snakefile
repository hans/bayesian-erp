from pathlib import Path
import itertools
import re
import yaml

from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider
HTTP = HTTPRemoteProvider()

# TODO config schema?

# Project root directory
root_dir = Path(workflow.basedir).parent

model = config["language_modeling"]["model"]


# # Glossary
#
# Task = stimulus = story. Tasks are indexed by integers and are ordered the same
# across subjects/sessions.
# TODO double-check this for all subjects+sessions
#
# # Other notes
#
# - NB that the general variable order for naming files here is
#   `task`, `subject`, `session`. This matches the Berp standard, where subject-session-level
#   data are categorized within stimulus.

# Read specified subjects/sessions/tasks for loading
SUBJECTS, SESSIONS = config['subjects'], config['sessions']
TASKS = config['tasks'].keys()
STORY_NAMES = config['tasks'].values()


wildcard_constraints:
    subject = "\d+",
    session = "\d+",
    task = "\d+",
    story_name = "\w+"


def take_first(expr, **wildcards):
    """
    Produce inputs drawing the first element of each of the given `wildcards`.
    This is useful when using denormalized data and we don't care which of the inputs we
    use -- we just need some reference.
    """
    return expand(expr, **{wildcard: values[0] for wildcard, values in wildcards.items()})


task_to_story_name = config["tasks"]
story_name_to_task = {v: k for k, v in task_to_story_name.items()}


def f(*args, **kwargs):
    print(args)
    print(kwargs)
    sys.exit(1)

# Extract data about the presentation of a task to a subject in a given session.
# Each "task" corresponds to a stimulus (story). NB the resulting extracted presentation
# data is highly denormalized, containing redundant information about phonemes and words
# that are repeated across stories.
#
# The unique data in each run of this rule is simply the timing of the stimulus presentation
# relative to the subject-session's EEG time series.
rule extract_presentation_data:
    input:
        lambda wildcards:
            expand("raw-data/sub-{{subject}}/ses-{{session}}/meg/sub-{{subject}}_ses-{{session}}_task-{task}_meg.con",
                   task=story_name_to_task[wildcards.story_name],
                   allow_missing=True)
    output:
        output_dir = directory("data/presentation/{story_name}/{subject}/{session}"),
        sounds = "data/presentation/{story_name}/{subject}/{session}/sound.csv",
        words = "data/presentation/{story_name}/{subject}/{session}/word.csv",
        phonemes = "data/presentation/{story_name}/{subject}/{session}/phoneme.csv"

    # TODO after this step, there should be a check/assertion that the time series
    # are uniform between subjects after baselining
    # At the same time, save the offsets relevant for aligning things later on.
    shell:
        """
        papermill scripts/meg-masc/extract_presentation_data.ipynb /dev/null \
            -p story_name {wildcards.story_name} \
            -p meg_path {input} \
            -p output_dir {output.output_dir}
        """


rule tokenized:
    input: "data/raw-text/{story_name}.txt"
    output: "data/tokenized/{model}/{story_name}.txt"
    shell:
        """
        papermill scripts/meg-masc/tokenize.ipynb /dev/null \
            -p model {wildcards.model} \
            -p input_path {input} \
            -p output_path {output}
        """


# Match up force-aligned corpus with model-tokenized corpus.
# This will allow us to use token-level features computed on the latter corpus
# together with the presentation data from the former.
rule align_with_tokens:
    input:
        presentation_words = lambda _:
            take_first("data/presentation/{{story_name}}/{subject}/{session}/word.csv",
                          subject=SUBJECTS, session=SESSIONS),
        presentation_phonemes = lambda _:
            take_first("data/presentation/{{story_name}}/{subject}/{session}/phoneme.csv",
                          subject=SUBJECTS, session=SESSIONS),
        tokenized = "data/tokenized/{model}/{story_name}.txt"
    output:
        aligned_directory = directory("data/aligned/{model}/{story_name}"),
        aligned_words = "data/aligned/{model}/{story_name}/word.csv",
        aligned_phonemes = "data/aligned/{model}/{story_name}/phoneme.csv"
    shell:
        """
        papermill scripts/meg-masc/align_with_tokens.ipynb /dev/null \
            -p presentation_words_path {input.presentation_words} \
            -p presentation_phonemes_path {input.presentation_phonemes} \
            -p tokenized_path {input.tokenized} \
            -p story_name {wildcards.story_name} \
            -p output_dir {output.aligned_directory}
        """


rule fetch_frequency_data:
    input:
        HTTP.remote("ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexus/subtlexus2.zip")
    output:
        "data/frequency/subtlexus2.csv"
    shell:
        "unzip -p {input} SUBTLEXus74286wordstextversion.txt > {output}"


# Run a language model on the resulting aligned text inputs and generate a
# NaturalLanguageStimulus, representing word- and phoneme-level prior predictive
# distributions.
rule run_language_modeling:
    input:
        tokenized = "data/tokenized/{model}/{story_name}.txt",
        aligned_words = "data/aligned/{model}/{story_name}/word.csv",
        aligned_phonemes = "data/aligned/{model}/{story_name}/phoneme.csv",

        cmu_ipa_dict = HTTP.remote("github.com/menelik3/cmudict-ipa/raw/master/cmudict-0.7b-ipa.txt"),
        frequency_data = "data/frequency/subtlexus2.csv"
    output:
        stimulus = "data/stimulus/{model}/{story_name}.pkl"
    shell:
        """
        papermill scripts/meg-masc/run_language_model.ipynb /dev/null \
            -p tokenized_path {input.tokenized} \
            -p aligned_words_path {input.aligned_words} \
            -p aligned_phonemes_path {input.aligned_phonemes} \
            -p cmu_ipa_dict_path {input.cmu_ipa_dict} \
            -p vocab_path {input.frequency_data} \
            -p story_name {wildcards.story_name} \
            -p model {wildcards.model} \
            -p output_path {output.stimulus}
        """


# TODO use computed offsets before to align stimulus+EEG data between subject-sessions
# Compute alignments of different stimulus presentations/recording sessions, so
# that stimuli/responses can be unified across sessions.
rule align_sessions:
    input:
        presentation_sounds = expand(
            "data/presentation/{story_name}/{subject}/{session}/sound.csv",
            story_name=STORY_NAMES, subject=SUBJECTS, session=SESSIONS,
            allow_missing=True),
        presentation_words = expand(
            "data/presentation/{story_name}/{subject}/{session}/word.csv",
            story_name=STORY_NAMES, subject=SUBJECTS, session=SESSIONS,
            allow_missing=True),
        presentation_phonemes = expand(
            "data/presentation/{story_name}/{subject}/{session}/phoneme.csv",
            story_name=STORY_NAMES, subject=SUBJECTS, session=SESSIONS,
            allow_missing=True),

    output:
        "data/session_alignment.csv"

    run:
        presentations = [
            {
                "key": dict(zip(["subject", "session", "story_name"],
                                re.findall(r"data/presentation/([^/]+)/([^/]+)/([^/]+)", sound)[0])),
                "sound": sound, "word": word, "phoneme": phoneme
            }
            for sound, word, phoneme
            in zip(input.presentation_sounds,
                   input.presentation_words,
                   input.presentation_phonemes)]
        yaml_config = yaml.safe_dump(
            {"presentations": presentations,
             "output_path": output[0]})

        shell(f"""
            papermill scripts/meg-masc/align_sessions.ipynb /dev/null -y "{yaml_config}"
        """)


# Produce a BerpDataset from the aligned corpora for a single subject/session.
rule produce_dataset:
    params:
        task = lambda wildcards: story_name_to_task[wildcards.story_name]
    input:
        presentation_words = "data/presentation/{story_name}/{subject}/{session}.word.csv",
        presentation_phonemes = "data/presentation/{story_name}/{subject}/{session}.phoneme.csv",

        aligned_words = "data/aligned/{model}/{story_name}/word.csv",
        aligned_phonemes = "data/aligned/{model}/{story_name}/phoneme.csv",

        stimulus = "data/stimulus/{model}/{story_name}.pkl",

        bids = lambda wildcards:
            expand("raw-data/sub-{{subject}}/ses-{{session}}/meg/sub-{{subject}}_ses-{{session}}_task-{task}_meg.con",
                   task=story_name_to_task[wildcards.story_name])
    output:
        dataset = "data/dataset/{model}/{story_name}/{subject}/{session}.pkl"


# Average EEG response time series for colection of aligned subject-session datasets.
rule produce_average_dataset:
    input:
        datasets = expand("data/dataset/{model}/{story_name}/{subject}/{session}.pkl",
                          subject=SUBJECTS, session=SESSIONS, allow_missing=True)
    output:
        dataset = "data/dataset/{model}/{story_name}/average.pkl"
    script: "{root_dir}/scripts/meg-masc/average_datasets.py"