from pathlib import Path
import itertools
import json
import re
import yaml

from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider
HTTP = HTTPRemoteProvider()

# TODO config schema?

# Project root directory
root_dir = Path(workflow.basedir).parent

model = config["language_modeling"]["model"]

# Strip this from all EEG data files when computing subject names
EEG_SUFFIX = "_1_256_8_average_4_128.mat"


# Read specified subjects/stories for loading
SUBJECTS = config['subjects']
STORIES = config['stories']


wildcard_constraints:
    subject = "[\w_]+",
    story = "[\w_]+",
    group_average_strategy = "macro|micro"


def take_first(expr, **wildcards):
    """
    Produce inputs drawing the first element of each of the given `wildcards`.
    This is useful when using denormalized data and we don't care which of the inputs we
    use -- we just need some reference.
    """
    return expand(expr, **{wildcard: values[0] for wildcard, values in wildcards.items()})


def f(*args, **kwargs):
    print(args)
    print(kwargs)
    sys.exit(1)


# Extract word/phoneme timing information from Praat TextGrid annotation.
rule convert_textgrid:
    input:
        "raw_data/textgrids/{story}.TextGrid"
    output:
        "data/presentations/{story}.csv"
    shell:
        "python scripts/gillis2021/convert_textgrid.py {input} > {output}"


# Convert Gillis' features from Eelbrain representation to numpy representation.
rule convert_stimulus_features:
    input:
        "raw_data/predictors"
    output:
        "data/stimuli.npz"
    conda:
        "envs/eelbrain.yml"
    shell:
        """
        python scripts/gillis2021/convert_features.py {input} {output}
        """


# Match up force-aligned corpus with raw text corpus on a token-to-token level.
# This will allow us to use token-level features computed on the latter corpus
# together with the timing data from the former.
# 
# Outputs a tokenized version of the raw text and a CSV describing the alignment
# between this tokenized version and the force-aligned corpus.
rule align_with_raw_text:
    input:
        presentation = "data/presentations/{story}.csv",
        raw_text = "raw_data/raw_text/{story}.txt"
    output:
        tokenized = "data/tokenized/{model}/{story}.txt",
        words = "data/aligned/{model}/{story}/word.csv",
        phonemes = "data/aligned/{model}/{story}/phoneme.csv"
    shell:
        """
        python scripts/gillis2021/align_with_raw_text.py \
            -m {wildcards.model} \
            {input.raw_text} {input.presentation} \
            {output.tokenized} {output.words} {output.phonemes}
        """


# Run a language model on the resulting aligned text inputs and generate a
# NaturalLanguageStimulus, representing word- and phoneme-level prior predictive
# distributions.
rule run_language_modeling:
    input:
        tokenized = "data/tokenized/{model}/{story_name}.txt",
        aligned_words = "data/aligned/{model}/{story_name}/word.csv",
        aligned_phonemes = "data/aligned/{model}/{story_name}/phoneme.csv",

        vocab = "raw_data/vocab.pkl",
        celex = "raw_data/celex_dpw_cx.txt"
    output:
        stimulus = "data/stimulus/{model}/{story_name}.pkl"
    run:
        shell("""
        export PYTHONPATH=.
        python scripts/gillis2021/run_language_model.py \
            -m {wildcards.model} \
            -n {config[language_modeling][n_candidates]} \
            --vocab_path {input.vocab} \
            --celex_path {input.celex} \
            -o {output.stimulus} \
            {input.tokenized} {input.aligned_words} {input.aligned_phonemes}
        """)

    # TODO make a snakemake bug report: using `shell` here interpolates
    # the global config, not the updated config from the module setup.
    # so `n_candidates` is not found. Things look okay accessing `config`
    # from Python script above directly, or using the Snakemake interpolation
    # syntax
    # shell:
    #     """
    #     export PYTHONPATH=.
    #     python scripts/gillis2021/run_language_model.py \
    #         -m {wildcards.model} \
    #         -n {config[language_modeling][n_candidates]} \
    #         --vocab_path {input.vocab} \
    #         --celex_path {input.celex} \
    #         -o {output.stimulus} \
    #         {input.tokenized} {input.aligned_words} {input.aligned_phonemes}
    #     """


# Produce a BerpDataset from the aligned corpora for a single subject.
rule produce_dataset:
    input:
        aligned_words = "data/aligned/{model}/{story_name}/word.csv",
        aligned_phonemes = "data/aligned/{model}/{story_name}/phoneme.csv",

        stimulus = "data/stimulus/{model}/{story_name}.pkl",
        stim_features = "data/stimuli.npz",

        eeg_data = expand(
            "raw_data/eeg/{{story_name}}/{{subject}}{suffix}",
            suffix=EEG_SUFFIX)
    output:
        dataset = "data/dataset/{model}/{story_name}/{subject}.pkl"
    shell:
        """
        export PYTHONPATH=.
        python scripts/gillis2021/produce_dataset.py \
            -o {output.dataset} \
            {input.stimulus} \
            {input.aligned_words} {input.aligned_phonemes} \
            {input.eeg_data} {input.stim_features}
        """


rule prepare_confusion_matrix:
    input:
        stimulus = lambda _: take_first(
            "data/stimulus/{model}/{story}.pkl",
            story=STORIES, model=[config["language_modeling"]["model"]]),

        confusion_data = "raw_data/confusion/phon2_conf_matrix_gate5.dat"
    output:
        confusion_matrix = "data/confusion_matrix.npz"
    shell:
        """
        export PYTHONPATH=.
        python scripts/gillis2021/prepare_confusion.py \
            -o {output.confusion_matrix} \
            {input.confusion_data} {input.stimulus}
        """


# Average EEG response time series for collection of aligned subject-story datasets.
rule produce_average_dataset:
    input:
        datasets = expand("data/dataset/{model}/{story_name}/{subject}.pkl",
                          subject=SUBJECTS, allow_missing=True)
    params:
        method_name = "{group_average_strategy}average"
    output:
        dataset = "data/dataset/{model}/{story_name}/{group_average_strategy}average.pkl"
    
    run:
        if config["subset_sensors"]:
            subset_sensor_str = "--subset_sensors " + ",".join(config["subset_sensors"])
        else:
            subset_sensor_str = ""
        
        shell("""
        export PYTHONPATH=.
        python scripts/gillis2021/produce_average_dataset.py \
            -o {output.dataset} \
            -n {wildcards.story_name}/{params.method_name} \
            {subset_sensor_str} \
            --average_strategy {wildcards.group_average_strategy} \
            {input.datasets}
        """)


def hydra_param(obj):
    """
    Prepare the given object for use as a Hydra CLI override.
    """
    if isinstance(obj, snakemake.io.Namedlist):
        obj = list(obj)
    return yaml.safe_dump(obj, default_flow_style=True, width=float("inf")).strip()


# Fit a "base baseline" model which does not include any variable-onset features (no
# word surprisal or word frequency).
rule fit_base_baseline_model:
    input:
        # datasets = expand(
        #     "data/dataset/{{model}}/{story_name}/{subject}.pkl",
        #     story_name=STORIES, subject=SUBJECTS, allow_missing=True),
        # Fit average.
        datasets = expand(
            "data/dataset/{{model}}/{story_name}/{group_average_strategy}average.pkl",
            story_name=STORIES, allow_missing=True),
        stimuli = expand(
            "data/stimulus/{{model}}/{story_name}.pkl",
            story_name=STORIES),
        confusion = "data/confusion_matrix.npz"

    params:
        berp_model = "trf"

    output:
        trace = directory("results_{group_average_strategy}average/{model}/base_trf")

    run:
        dataset_path_str = hydra_param(input.datasets)

        stimulus_dict = {
            Path(stimulus).stem: stimulus for stimulus in input.stimuli
        }
        stimulus_dict_str = hydra_param(stimulus_dict)

        dataset_spec = f"dkz_{wildcards.group_average_strategy}average"
        cv_spec = config["fit"]["cv_strategy"][params.berp_model]

        shell("""
        export PYTHONPATH=.
        python scripts/fit_em.py \
            model={params.berp_model} \
            features=dkz \
            'features.variable_feature_names=[]' \
            dataset={dataset_spec} \
            'dataset.paths={dataset_path_str}' \
            +dataset.stimulus_paths='{stimulus_dict_str}' \
            cv={cv_spec} \
            solver=adam \
            hydra.run.dir={output.trace}
        """)


rule fit_baseline_model:
    input:
        # datasets = expand(
        #     "data/dataset/{{model}}/{story_name}/{subject}.pkl",
        #     story_name=STORIES, subject=SUBJECTS, allow_missing=True),
        # Fit average.
        datasets = expand(
            "data/dataset/{{model}}/{story_name}/{group_average_strategy}average.pkl",
            story_name=STORIES, allow_missing=True),
        stimuli = expand(
            "data/stimulus/{{model}}/{story_name}.pkl",
            story_name=STORIES),
        confusion = "data/confusion_matrix.npz",

        baseline_model = "results_{group_average_strategy}average/{model}/base_trf"

    params:
        berp_model = "trf"

    output:
        trace = directory("results_{group_average_strategy}average/{model}/trf")

    run:
        dataset_path_str = hydra_param(input.datasets)

        stimulus_dict = {
            Path(stimulus).stem: stimulus for stimulus in input.stimuli
        }
        stimulus_dict_str = hydra_param(stimulus_dict)

        dataset_spec = f"dkz_{wildcards.group_average_strategy}average"
        cv_spec = config["fit"]["cv_strategy"][params.berp_model]

        shell("""
        export PYTHONPATH=.
        python scripts/fit_em.py \
            model={params.berp_model} \
            baseline_model_path={input.baseline_model} \
            features=dkz \
            'features.variable_feature_names=[word_frequency, word_surprisal]' \
            dataset={dataset_spec} \
            'dataset.paths={dataset_path_str}' \
            +dataset.stimulus_paths='{stimulus_dict_str}' \
            cv={cv_spec} \
            solver=adam \
            hydra.run.dir={output.trace}
        """)


rule fit_model:
    input:
        # datasets = expand(
        #     "data/dataset/{{model}}/{story_name}/{subject}.pkl",
        #     story_name=STORIES, subject=SUBJECTS, allow_missing=True),
        datasets = expand(
            "data/dataset/{{model}}/{story_name}/{group_average_strategy}average.pkl",
            story_name=STORIES, allow_missing=True),
        stimuli = expand(
            "data/stimulus/{{model}}/{story_name}.pkl",
            story_name=STORIES),
        confusion = "data/confusion_matrix.npz",

        baseline_model = "results_{group_average_strategy}average/{model}/trf"

    output:
        trace = directory("results_{group_average_strategy}average/{model}/{berp_model}")

    run:
        dataset_path_str = hydra_param(input.datasets)

        stimulus_dict = {
            Path(stimulus).stem: stimulus for stimulus in input.stimuli
        }
        stimulus_dict_str = hydra_param(stimulus_dict)

        dataset_spec = f"dkz_{wildcards.group_average_strategy}average"
        cv_spec = config["fit"]["cv_strategy"][wildcards.berp_model]

        shell("""
        export PYTHONPATH=.
        python scripts/fit_em.py \
            model={wildcards.berp_model} \
            baseline_model_path={input.baseline_model} \
            features=dkz \
            dataset={dataset_spec} \
            'dataset.paths={dataset_path_str}' \
            +dataset.stimulus_paths='{stimulus_dict_str}' \
            model.confusion_path={input.confusion} \
            cv={cv_spec} \
            solver=adam \
            hydra.run.dir={output.trace}
        """)