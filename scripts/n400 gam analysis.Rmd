---
title: "n400 gam analysis"
output:
  html_notebook: default
params:
  data_path: "n400_gam_data.csv"
---

```{r setup, include=FALSE}
library(data.table)
library(ggplot2)
library(lme4)
library(mgcv)
library(tidyverse)
library(broom)

knitr::opts_chunk$set(echo = TRUE)
```

```{r}
df = read_csv(params$data_path,
              col_types=list(
                story=col_factor(),
                subject=col_factor(),
                run=col_factor(),
                word_idx=col_factor()
              )) %>% 
  mutate(word_surprisal=-word_surprisal)
glimpse(df)
```

```{r}
# DEV
reg_data = df %>%
  mutate(item=as.factor(paste(run, word_idx)),
         # word_surprisal=c(scale(word_surprisal, scale=F)),
         # word_frequency=c(scale(word_frequency, scale=F)),
         baseline=c(scale(baseline, scale=F)))
nrow(reg_data)
```


```{r}
# linear_model = lmer(n400 ~ baseline + word_surprisal + word_frequency +
#                       (baseline + word_surprisal + word_frequency | subject),# +
#                       #(1 | item),
#                     data=reg_data %>% filter(sensor_name == "B4"), REML=F, control=lmerControl(optimizer="bobyqa"))
# summary(linear_model)
```

## GAM analysis

```{r}
my_gam_predict = function(gm, view) {
  m1 <- seq(min(gm$var.summary[[view[1]]], na.rm = TRUE), max(gm$var.summary[[view[1]]], na.rm = TRUE), length = 30)
  cond = list()
  cond[[view[1]]] <- m1
  
  newd = as.data.table(itsadug::get_predictions(gm, cond=cond, se=T, f=1.96, sim.ci=F, rm.ranef=T, print.summary=T))
  newd[, ul:=fit + CI]
  newd[, ll:=fit - CI]

  return (newd)
}
```

```{r}
# gam = bam(n400 ~ s(subject, bs="re") + #s(item, bs="re") +
#             baseline + s(baseline, bs="cr", m=c(2, 0)) + s(subject, baseline, bs="re") + #s(item, baseline, bs="re") +
#             word_surprisal + s(word_surprisal, bs="cr", m=c(2, 0)) + s(subject, word_surprisal, bs="re") +
#             word_frequency + s(word_frequency, bs="cr", m=c(2, 0)) + s(subject, word_frequency, bs="re"),
#           data=reg_data %>% filter(sensor_name == "B4"))
# summary(gam)

# plotret = my_gam_predict(gam, "word_surprisal")
# setnames(plotret, "word_surprisal", "pred")

# ggplot(plotret, aes(x=pred, y=fit, ymin=ll, ymax=ul)) +
#   geom_line() + geom_ribbon(alpha=0.2) +
#   xlab("Surprisal") +
#   ylab("N400 amplitude")
```

```{r}
all_gams = reg_data %>%
  group_by(sensor_name) %>% 
  nest() %>% 
    mutate(fit=map(data,
                   ~bam(n400 ~ s(subject, bs="re") + #s(item, bs="re") +
                          baseline + s(baseline, bs="cr", m=c(2, 0)) + s(subject, baseline, bs="re") + #s(item, baseline, bs="re") +
                          word_surprisal + s(word_surprisal, bs="cr", m=c(2, 0)) + s(subject, word_surprisal, bs="re") +
                          word_frequency + s(word_frequency, bs="cr", m=c(2, 0)) + s(subject, word_frequency, bs="re"),
                        data=.x, nthreads=4)))
```

```{r}
gam_smooth_results = all_gams %>%
  mutate(tidied=map(fit, tidy)) %>% 
  select(-data, -fit) %>% 
  unnest(tidied)
gam_smooth_results %>%
  filter(term == "s(word_surprisal)") %>% 
  arrange(p.value) %>% 
  mutate(sig=p.value < 0.01)
```


```{r fig.width=15, fig.align="center"}
all_gams %>% mutate(predict=map(fit, ~my_gam_predict(.x, "word_surprisal"))) %>% 
  select(-fit, -data) %>% 
  unnest(predict) %>% 
  ggplot(aes(x=word_surprisal, y=fit, ymin=ll, ymax=ul)) +
    facet_wrap(~sensor_name, ncol=4) +
    geom_line() + geom_ribbon(alpha=0.2) +
    xlab("Surprisal") +
    ylab("N400 amplitude")
```

### GAM with lower regularization

See if we can get the output to match the piecewise linear analysis ..



## Piecewise linear analysis

```{r}
# Surprisal quantile edges as estimated by prior-split model on the train dataset.
surprisal_quantile_breaks = c(0, 1.3311, 3.7081, max(df$word_surprisal) + 1)
surprisal_quantile_labels = as.factor(c(0, 1, 2))

# Or .. re-estimate quantiles, possibly a different number
n_quantiles = 5
surprisal_quantile_breaks = quantile(df$word_surprisal, 0:n_quantiles/n_quantiles)
surprisal_quantile_labels = as.factor(0:(n_quantiles - 1))

piecewise_linear_fits = df %>%
  mutate(quantile=cut(word_surprisal, breaks=surprisal_quantile_breaks, labels=surprisal_quantile_labels, include.lowest = T)) %>% 
  group_by(sensor_name, quantile) %>% 
  nest() %>% 
    mutate(fit=map(data, ~lm(n400 ~ baseline + word_surprisal + word_frequency, data=.x)))

piecewise_linear_estimates = piecewise_linear_fits %>% 
  mutate(tidied=map(fit, tidy)) %>% 
  select(-data, -fit) %>% 
  unnest(tidied)
```

```{r}
piecewise_linear_intercepts = piecewise_linear_estimates %>% filter(term == "(Intercept)")
piecewise_linear_slopes = piecewise_linear_estimates %>% filter(term == "word_surprisal")

piecewise_linear_intercepts %>% filter(is.na(quantile))

data.frame(quantile=surprisal_quantile_labels, quantile_left=head(surprisal_quantile_breaks, n=-1), quantile_right=tail(surprisal_quantile_breaks, n=-1)) %>% 
  mutate(word_surprisal=map2(quantile_left, quantile_right, ~seq(.x, .y, length=2))) %>%
  unnest(word_surprisal) %>% 
  right_join(piecewise_linear_intercepts[, c("sensor_name", "quantile", "estimate")] %>% rename(intercept="estimate"), by="quantile") %>%
  left_join(piecewise_linear_slopes[, c("sensor_name", "quantile", "estimate")] %>% rename(slope="estimate"), by=c("quantile", "sensor_name")) %>%
  mutate(y=intercept + slope * word_surprisal) %>% 
  ggplot(aes(x=word_surprisal, y=y, color=quantile)) +
    geom_line() +
    facet_wrap(~sensor_name, ncol=4)
```


```{r}
piecewise_linear_estimates %>% 
  filter(term == "word_surprisal") %>% 
  arrange(p.value)
```


```{r}
piecewise_linear_estimates %>% 
  filter(term == "word_surprisal") %>% 
  group_by(sensor_name, quantile) %>% 
    summarise(estimate=mean(estimate))
```

```{r}
piecewise_linear_fits2 = df %>%
  mutate(quantile=cut(word_surprisal, breaks=c(-Inf, -3.7081, -1.3311, Inf), labels=c(0, 1, 2)),
         word_surprisal=scale(-word_surprisal)) %>% 
  group_by(sensor_name, quantile) %>% 
  nest() %>% 
    mutate(fit=map(data, ~lm(n400 ~ baseline + word_surprisal + word_frequency, data=.x)),
           tidied=map(fit, tidy)) %>% 
  select(-data, -fit) %>% 
  unnest(tidied)
```

```{r}
piecewise_linear_fits2 %>% 
  filter(term == "word_surprisal") %>% 
  group_by(sensor_name, quantile) %>% 
    summarise(estimate=mean(estimate))
```


