{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dfedbb1-b4df-49f8-9deb-859940589d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "from colorama import Fore, Style\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45f2e7f3-3bf9-4d7e-8d92-949826147f3e",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "presentation_words_path = \"./lw1.01.0.word.csv\"\n",
    "presentation_phonemes_path = \"./lw1.01.0.phoneme.csv\"\n",
    "tokenized_path = \"./lw1.tokenized.txt\"\n",
    "story_name = \"lw1\"\n",
    "output_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787b767c-340b-454f-8f65-2dd5e29737f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = Path(tokenized_path).read_text().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f9fc04e-bb52-4f02-b8f8-ae9ce029cba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'ĠHarmon',\n",
       " \"'s\",\n",
       " 'Ġface',\n",
       " 'Ġslowly',\n",
       " 'Ġanimated',\n",
       " 'ĠâĢĶ',\n",
       " 'Ġjoy',\n",
       " 'Ġsweeping',\n",
       " 'Ġin']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO fix encoding mistake in tokenizer. it's the em dash\n",
    "tokens[95:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdd3628b-1fe3-4b19-aede-d00a790eebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.read_csv(presentation_words_path, index_col=0)\n",
    "phonemes_df = pd.read_csv(presentation_phonemes_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c1e820b-2e8d-4209-9db1-02bf63d5b434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>story_uid</th>\n",
       "      <th>sound_id</th>\n",
       "      <th>kind</th>\n",
       "      <th>meg_file</th>\n",
       "      <th>start</th>\n",
       "      <th>sound</th>\n",
       "      <th>word</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>condition</th>\n",
       "      <th>word_index</th>\n",
       "      <th>speech_rate</th>\n",
       "      <th>voice</th>\n",
       "      <th>pronounced</th>\n",
       "      <th>onset</th>\n",
       "      <th>duration</th>\n",
       "      <th>value</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>stimuli/audio/lw1_0.wav</td>\n",
       "      <td>Tara</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.506</td>\n",
       "      <td>0.30</td>\n",
       "      <td>697</td>\n",
       "      <td>23506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>stimuli/audio/lw1_0.wav</td>\n",
       "      <td>stood</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>1.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.816</td>\n",
       "      <td>0.24</td>\n",
       "      <td>698</td>\n",
       "      <td>23816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>stimuli/audio/lw1_0.wav</td>\n",
       "      <td>stock</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>2.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.056</td>\n",
       "      <td>0.37</td>\n",
       "      <td>699</td>\n",
       "      <td>24056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>stimuli/audio/lw1_0.wav</td>\n",
       "      <td>still</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>3.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.586</td>\n",
       "      <td>0.40</td>\n",
       "      <td>700</td>\n",
       "      <td>24586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>1.630000</td>\n",
       "      <td>stimuli/audio/lw1_0.wav</td>\n",
       "      <td>waiting</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>4.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.136</td>\n",
       "      <td>0.41</td>\n",
       "      <td>701</td>\n",
       "      <td>25136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>50.070000</td>\n",
       "      <td>stimuli/audio/lw1_3.wav</td>\n",
       "      <td>end</td>\n",
       "      <td>52.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>15.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>361.097</td>\n",
       "      <td>0.17</td>\n",
       "      <td>3119</td>\n",
       "      <td>361097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>50.250000</td>\n",
       "      <td>stimuli/audio/lw1_3.wav</td>\n",
       "      <td>for</td>\n",
       "      <td>52.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>16.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>361.277</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3120</td>\n",
       "      <td>361277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>50.460000</td>\n",
       "      <td>stimuli/audio/lw1_3.wav</td>\n",
       "      <td>project</td>\n",
       "      <td>52.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>18.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>361.487</td>\n",
       "      <td>0.58</td>\n",
       "      <td>3121</td>\n",
       "      <td>361487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>51.179999</td>\n",
       "      <td>stimuli/audio/lw1_3.wav</td>\n",
       "      <td>and</td>\n",
       "      <td>52.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>19.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>362.207</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3122</td>\n",
       "      <td>362207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>51.790000</td>\n",
       "      <td>stimuli/audio/lw1_3.wav</td>\n",
       "      <td>species</td>\n",
       "      <td>52.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>22.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>362.817</td>\n",
       "      <td>0.34</td>\n",
       "      <td>3123</td>\n",
       "      <td>362817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>668 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    story  story_uid  sound_id  kind                     meg_file      start  \\\n",
       "0     lw1        0.0       0.0  word  A0167_MASC_1_16Mar17_01.con   0.000000   \n",
       "1     lw1        0.0       0.0  word  A0167_MASC_1_16Mar17_01.con   0.310000   \n",
       "2     lw1        0.0       0.0  word  A0167_MASC_1_16Mar17_01.con   0.550000   \n",
       "3     lw1        0.0       0.0  word  A0167_MASC_1_16Mar17_01.con   1.080000   \n",
       "4     lw1        0.0       0.0  word  A0167_MASC_1_16Mar17_01.con   1.630000   \n",
       "..    ...        ...       ...   ...                          ...        ...   \n",
       "663   lw1        0.0       3.0  word  A0167_MASC_1_16Mar17_01.con  50.070000   \n",
       "664   lw1        0.0       3.0  word  A0167_MASC_1_16Mar17_01.con  50.250000   \n",
       "665   lw1        0.0       3.0  word  A0167_MASC_1_16Mar17_01.con  50.460000   \n",
       "666   lw1        0.0       3.0  word  A0167_MASC_1_16Mar17_01.con  51.179999   \n",
       "667   lw1        0.0       3.0  word  A0167_MASC_1_16Mar17_01.con  51.790000   \n",
       "\n",
       "                       sound     word  sequence_id condition  word_index  \\\n",
       "0    stimuli/audio/lw1_0.wav     Tara          0.0  sentence         0.0   \n",
       "1    stimuli/audio/lw1_0.wav    stood          0.0  sentence         1.0   \n",
       "2    stimuli/audio/lw1_0.wav    stock          0.0  sentence         2.0   \n",
       "3    stimuli/audio/lw1_0.wav    still          0.0  sentence         3.0   \n",
       "4    stimuli/audio/lw1_0.wav  waiting          0.0  sentence         4.0   \n",
       "..                       ...      ...          ...       ...         ...   \n",
       "663  stimuli/audio/lw1_3.wav      end         52.0  sentence        15.0   \n",
       "664  stimuli/audio/lw1_3.wav      for         52.0  sentence        16.0   \n",
       "665  stimuli/audio/lw1_3.wav  project         52.0  sentence        18.0   \n",
       "666  stimuli/audio/lw1_3.wav      and         52.0  sentence        19.0   \n",
       "667  stimuli/audio/lw1_3.wav  species         52.0  sentence        22.0   \n",
       "\n",
       "     speech_rate    voice  pronounced    onset  duration  value  sample  \n",
       "0          205.0  Allison         1.0   23.506      0.30    697   23506  \n",
       "1          205.0  Allison         1.0   23.816      0.24    698   23816  \n",
       "2          205.0  Allison         1.0   24.056      0.37    699   24056  \n",
       "3          205.0  Allison         1.0   24.586      0.40    700   24586  \n",
       "4          205.0  Allison         1.0   25.136      0.41    701   25136  \n",
       "..           ...      ...         ...      ...       ...    ...     ...  \n",
       "663        205.0  Allison         1.0  361.097      0.17   3119  361097  \n",
       "664        205.0  Allison         1.0  361.277      0.14   3120  361277  \n",
       "665        205.0  Allison         1.0  361.487      0.58   3121  361487  \n",
       "666        205.0  Allison         1.0  362.207      0.15   3122  362207  \n",
       "667        205.0  Allison         1.0  362.817      0.34   3123  362817  \n",
       "\n",
       "[668 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "956767c3-3361-4aca-9f24-046e27f61a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_story(words_df, story_name):\n",
    "    # Ignore word list content.\n",
    "    words_df = words_df[~(words_df.condition == \"word_list\")]\n",
    "    # Ignore pseudowords.\n",
    "    # TODO maybe also ignore following N?\n",
    "    words_df = words_df[~(words_df.condition == \"pseudo_words\")]\n",
    "    \n",
    "    if story_name == \"lw1\":\n",
    "        # Typo fix\n",
    "        assert words_df.loc[571, \"word\"] == \"It's\"\n",
    "        words_df.loc[571, \"word\"] = \"Its\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown story {story_name}.\")\n",
    "        \n",
    "    return words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dcbb407-dce6-4a32-af76-b6d09b229464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>story_uid</th>\n",
       "      <th>sound_id</th>\n",
       "      <th>kind</th>\n",
       "      <th>meg_file</th>\n",
       "      <th>start</th>\n",
       "      <th>sound</th>\n",
       "      <th>word</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>condition</th>\n",
       "      <th>word_index</th>\n",
       "      <th>speech_rate</th>\n",
       "      <th>voice</th>\n",
       "      <th>pronounced</th>\n",
       "      <th>onset</th>\n",
       "      <th>duration</th>\n",
       "      <th>value</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>stimuli/audio/lw1_0.wav</td>\n",
       "      <td>Tara</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.506</td>\n",
       "      <td>0.30</td>\n",
       "      <td>697</td>\n",
       "      <td>23506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>stimuli/audio/lw1_0.wav</td>\n",
       "      <td>stood</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>1.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.816</td>\n",
       "      <td>0.24</td>\n",
       "      <td>698</td>\n",
       "      <td>23816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>stimuli/audio/lw1_0.wav</td>\n",
       "      <td>stock</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>2.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.056</td>\n",
       "      <td>0.37</td>\n",
       "      <td>699</td>\n",
       "      <td>24056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>stimuli/audio/lw1_0.wav</td>\n",
       "      <td>still</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>3.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.586</td>\n",
       "      <td>0.40</td>\n",
       "      <td>700</td>\n",
       "      <td>24586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>1.630000</td>\n",
       "      <td>stimuli/audio/lw1_0.wav</td>\n",
       "      <td>waiting</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>4.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.136</td>\n",
       "      <td>0.41</td>\n",
       "      <td>701</td>\n",
       "      <td>25136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>50.070000</td>\n",
       "      <td>stimuli/audio/lw1_3.wav</td>\n",
       "      <td>end</td>\n",
       "      <td>52.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>15.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>361.097</td>\n",
       "      <td>0.17</td>\n",
       "      <td>3119</td>\n",
       "      <td>361097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>50.250000</td>\n",
       "      <td>stimuli/audio/lw1_3.wav</td>\n",
       "      <td>for</td>\n",
       "      <td>52.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>16.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>361.277</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3120</td>\n",
       "      <td>361277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>50.460000</td>\n",
       "      <td>stimuli/audio/lw1_3.wav</td>\n",
       "      <td>project</td>\n",
       "      <td>52.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>18.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>361.487</td>\n",
       "      <td>0.58</td>\n",
       "      <td>3121</td>\n",
       "      <td>361487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>51.179999</td>\n",
       "      <td>stimuli/audio/lw1_3.wav</td>\n",
       "      <td>and</td>\n",
       "      <td>52.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>19.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>362.207</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3122</td>\n",
       "      <td>362207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>51.790000</td>\n",
       "      <td>stimuli/audio/lw1_3.wav</td>\n",
       "      <td>species</td>\n",
       "      <td>52.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>22.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>362.817</td>\n",
       "      <td>0.34</td>\n",
       "      <td>3123</td>\n",
       "      <td>362817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>613 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    story  story_uid  sound_id  kind                     meg_file      start  \\\n",
       "0     lw1        0.0       0.0  word  A0167_MASC_1_16Mar17_01.con   0.000000   \n",
       "1     lw1        0.0       0.0  word  A0167_MASC_1_16Mar17_01.con   0.310000   \n",
       "2     lw1        0.0       0.0  word  A0167_MASC_1_16Mar17_01.con   0.550000   \n",
       "3     lw1        0.0       0.0  word  A0167_MASC_1_16Mar17_01.con   1.080000   \n",
       "4     lw1        0.0       0.0  word  A0167_MASC_1_16Mar17_01.con   1.630000   \n",
       "..    ...        ...       ...   ...                          ...        ...   \n",
       "663   lw1        0.0       3.0  word  A0167_MASC_1_16Mar17_01.con  50.070000   \n",
       "664   lw1        0.0       3.0  word  A0167_MASC_1_16Mar17_01.con  50.250000   \n",
       "665   lw1        0.0       3.0  word  A0167_MASC_1_16Mar17_01.con  50.460000   \n",
       "666   lw1        0.0       3.0  word  A0167_MASC_1_16Mar17_01.con  51.179999   \n",
       "667   lw1        0.0       3.0  word  A0167_MASC_1_16Mar17_01.con  51.790000   \n",
       "\n",
       "                       sound     word  sequence_id condition  word_index  \\\n",
       "0    stimuli/audio/lw1_0.wav     Tara          0.0  sentence         0.0   \n",
       "1    stimuli/audio/lw1_0.wav    stood          0.0  sentence         1.0   \n",
       "2    stimuli/audio/lw1_0.wav    stock          0.0  sentence         2.0   \n",
       "3    stimuli/audio/lw1_0.wav    still          0.0  sentence         3.0   \n",
       "4    stimuli/audio/lw1_0.wav  waiting          0.0  sentence         4.0   \n",
       "..                       ...      ...          ...       ...         ...   \n",
       "663  stimuli/audio/lw1_3.wav      end         52.0  sentence        15.0   \n",
       "664  stimuli/audio/lw1_3.wav      for         52.0  sentence        16.0   \n",
       "665  stimuli/audio/lw1_3.wav  project         52.0  sentence        18.0   \n",
       "666  stimuli/audio/lw1_3.wav      and         52.0  sentence        19.0   \n",
       "667  stimuli/audio/lw1_3.wav  species         52.0  sentence        22.0   \n",
       "\n",
       "     speech_rate    voice  pronounced    onset  duration  value  sample  \n",
       "0          205.0  Allison         1.0   23.506      0.30    697   23506  \n",
       "1          205.0  Allison         1.0   23.816      0.24    698   23816  \n",
       "2          205.0  Allison         1.0   24.056      0.37    699   24056  \n",
       "3          205.0  Allison         1.0   24.586      0.40    700   24586  \n",
       "4          205.0  Allison         1.0   25.136      0.41    701   25136  \n",
       "..           ...      ...         ...      ...       ...    ...     ...  \n",
       "663        205.0  Allison         1.0  361.097      0.17   3119  361097  \n",
       "664        205.0  Allison         1.0  361.277      0.14   3120  361277  \n",
       "665        205.0  Allison         1.0  361.487      0.58   3121  361487  \n",
       "666        205.0  Allison         1.0  362.207      0.15   3122  362207  \n",
       "667        205.0  Allison         1.0  362.817      0.34   3123  362817  \n",
       "\n",
       "[613 rows x 18 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df = patch_story(words_df, story_name)\n",
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7367c51-3c58-4bff-a630-ffa9013f240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (words_df[[\"story\", \"sequence_id\", \"word_index\"]].apply(lambda xs: \" \".join(map(str, xs)), axis=1).value_counts() == 1).all(), \\\n",
    "    \"(story, sequence_id, word_index) should identify a unique word in the presentation\"\n",
    "\n",
    "assert (words_df.story == story_name).all()\n",
    "assert (phonemes_df.story == story_name).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b2b6124-f6ff-4928-a743-b5a7adbeff8d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "only_punct_re = re.compile(r\"^[^A-zÀ-ž0-9]+$\")\n",
    "\n",
    "\n",
    "class Aligner:\n",
    "    \n",
    "    flag_types = {\n",
    "        \"recap\": 0,  # the word was repeated one or more times in the FA\n",
    "    }\n",
    "    \n",
    "    def __init__(self, tokens, words_df,\n",
    "                 max_skip_patience: int = 20):\n",
    "        self.tokens = tokens\n",
    "        self.words_df = words_df\n",
    "        \n",
    "        # Tracks alignment between indices in FA corpus (original, prior to filtering)\n",
    "        # and indices in tokens_flat. Third element indicates various metadata about\n",
    "        # alignment (see `flag_types`).\n",
    "        self.alignment: List[Tuple[int, int, int]] = []\n",
    "        # Track current transaction for modifying alignment\n",
    "        self.transaction = []\n",
    "        \n",
    "        self.tok_cursor = 0\n",
    "        self.word_cursor = 0\n",
    "        \n",
    "        self.max_skip_patience = max_skip_patience\n",
    "        self.skip_patience = self.max_skip_patience\n",
    "    \n",
    "    def process_token(self, token):\n",
    "        return token.replace(\"Ġ\", \"\").lower()\n",
    "\n",
    "    @property\n",
    "    def tok_el(self) -> str:\n",
    "        return self.process_token(self.tokens[self.tok_cursor])\n",
    "    \n",
    "    @property\n",
    "    def word_row(self) -> pd.Series:\n",
    "        return self.words_df.iloc[self.word_cursor]\n",
    "    \n",
    "    @property\n",
    "    def word_el(self) -> str:\n",
    "        return self.word_row.word.lower()\n",
    "    \n",
    "    @property\n",
    "    def word_index(self) -> int:\n",
    "        return self.words_df.index[self.word_cursor]\n",
    "    \n",
    "    def advance(self, first_delta=1):\n",
    "        next_token = None\n",
    "        while self.tok_cursor + 1 < len(self.tokens) and \\\n",
    "            (next_token is None or only_punct_re.match(next_token)):\n",
    "            self.tok_cursor += first_delta if next_token is None else 1\n",
    "\n",
    "            next_token = self.tok_el\n",
    "\n",
    "        # print(\"///\", tok_cursor, next_token)\n",
    "        \n",
    "    def start_transaction(self):\n",
    "        self._orig_tok_cursor = self.tok_cursor\n",
    "        self._orig_word_cursor = self.word_cursor\n",
    "        \n",
    "    def commit_transaction(self):\n",
    "        self.skip_patience = self.max_skip_patience\n",
    "        self.alignment += self.transaction\n",
    "        self.transaction = []\n",
    "        \n",
    "    def drop_transaction(self):\n",
    "        self.skip_patience = self.max_skip_patience\n",
    "        self.tok_cursor = self._orig_tok_cursor\n",
    "        self.word_cursor = self._orig_word_cursor\n",
    "        self.transaction = []\n",
    "        \n",
    "    def stage(self, flags=None, do_advance=True):\n",
    "        print(f\"{self.word_el} -- {self.tok_el}\")\n",
    "        self.transaction.append((self.word_index, self.tok_cursor, flags))\n",
    "\n",
    "        # Reset skip patience\n",
    "        self.skip_patience = self.max_skip_patience\n",
    "\n",
    "        # Advance cursor\n",
    "        if do_advance:\n",
    "            try:\n",
    "                self.advance()\n",
    "            except IndexError:\n",
    "                raise StopIteration\n",
    "                \n",
    "    def attempt_match(self) -> bool:\n",
    "        fa_el = self.word_el\n",
    "\n",
    "        if fa_el == self.tok_el:\n",
    "            self.start_transaction()\n",
    "            self.stage()\n",
    "            self.commit_transaction()\n",
    "            return True\n",
    "        elif fa_el.startswith(self.tok_el):\n",
    "            self.start_transaction()\n",
    "            while fa_el.startswith(self.tok_el):\n",
    "                fa_el = fa_el[len(self.tok_el):]\n",
    "                self.stage()\n",
    "                \n",
    "            if len(fa_el) > 0:\n",
    "                self.err(f\"Residual FA el {fa_el} not covered by token {self.tok_el}. Drop transaction.\")\n",
    "                print(self.skip_patience)\n",
    "                self.drop_transaction()\n",
    "                return False\n",
    "            else:\n",
    "                self.commit_transaction()\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def err(self, msg):\n",
    "        print(f\"{Fore.RED}{msg}{Style.RESET_ALL}\")\n",
    "        print(self.words_df.iloc[self.word_cursor - 5 : self.word_cursor + 5])\n",
    "        print(self.tokens[self.tok_cursor - 5 : self.tok_cursor + 5])\n",
    "        # raise ValueError(str((self.word_el, self.tok_el)))\n",
    "        \n",
    "    def __call__(self):\n",
    "        # Reset state\n",
    "        self.tok_cursor = 0\n",
    "        self.word_cursor = 0\n",
    "        self.skip_patience = self.max_skip_patience\n",
    "        self.alignment = []\n",
    "        \n",
    "        while True:\n",
    "            if self.word_cursor >= len(self.words_df):\n",
    "                return self.alignment\n",
    "\n",
    "            result = self.attempt_match()\n",
    "            if result:\n",
    "                self.word_cursor += 1 \n",
    "            elif self.skip_patience > 0:\n",
    "                # Try skipping this token and see if we find success in the near future.\n",
    "                print(f\"{Fore.YELLOW}Skipping token {self.tok_el}, didn't match with {self.word_el}{Style.RESET_ALL}\")\n",
    "                self.advance()\n",
    "                self.skip_patience -= 1\n",
    "            else:\n",
    "                self.err(\"Failed to find alignment. Stop.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "433a965e-d83d-4f5c-a4ff-cfba86acf413",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tara -- t\n",
      "tara -- ara\n",
      "stood -- stood\n",
      "stock -- stock\n",
      "still -- still\n",
      "waiting -- waiting\n",
      "for -- for\n",
      "the -- the\n",
      "first -- first\n",
      "tiny -- tiny\n",
      "gleam -- gle\n",
      "gleam -- am\n",
      "from -- from\n",
      "the -- the\n",
      "scout -- scout\n",
      "craft -- craft\n",
      "to -- to\n",
      "appear -- appear\n",
      "in -- in\n",
      "the -- the\n",
      "darkness -- darkness\n",
      "of -- of\n",
      "the -- the\n",
      "\u001b[33mSkipping token worm, didn't match with the\u001b[0m\n",
      "\u001b[33mSkipping token hole, didn't match with the\u001b[0m\n",
      "the -- the\n",
      "gentle -- gentle\n",
      "constant -- constant\n",
      "breeze -- breeze\n",
      "of -- of\n",
      "recycled -- recycled\n",
      "air -- air\n",
      "from -- from\n",
      "the -- the\n",
      "vent -- vent\n",
      "above -- above\n",
      "blew -- blew\n",
      "an -- an\n",
      "annoying -- annoying\n",
      "hair -- hair\n",
      "against -- against\n",
      "her -- her\n",
      "nose -- nose\n",
      "but -- but\n",
      "she -- she\n",
      "ignored -- ignored\n",
      "it -- it\n",
      "a -- a\n",
      "gasp -- gasp\n",
      "from -- from\n",
      "the -- the\n",
      "psychic -- psychic\n",
      "broke -- broke\n",
      "her -- her\n",
      "silent -- silent\n",
      "vigil -- vigil\n",
      "and -- and\n",
      "she -- she\n",
      "turned -- turned\n",
      "results -- results\n",
      "harmon -- harmon\n",
      "she -- she\n",
      "suppressed -- suppressed\n",
      "the -- the\n",
      "surge -- surge\n",
      "of -- of\n",
      "annoyance -- annoyance\n",
      "that -- that\n",
      "ran -- ran\n",
      "through -- through\n",
      "her -- her\n",
      "as -- as\n",
      "she -- she\n",
      "contemplated -- contemplated\n",
      "the -- the\n",
      "\u001b[33mSkipping token psi, didn't match with gift\u001b[0m\n",
      "\u001b[33mSkipping token 's, didn't match with gift\u001b[0m\n",
      "gift -- gift\n",
      "of -- of\n",
      "getting -- getting\n",
      "all -- all\n",
      "the -- the\n",
      "hot -- hot\n",
      "news -- news\n",
      "first -- first\n",
      "\u001b[33mSkipping token harmon, didn't match with face\u001b[0m\n",
      "\u001b[33mSkipping token 's, didn't match with face\u001b[0m\n",
      "face -- face\n",
      "slowly -- slowly\n",
      "animated -- animated\n",
      "\u001b[33mSkipping token âģķ, didn't match with joy\u001b[0m\n",
      "joy -- joy\n",
      "sweeping -- sweeping\n",
      "in -- in\n",
      "to -- to\n",
      "replace -- replace\n",
      "stern -- stern\n",
      "concentration -- concentration\n",
      "\u001b[33mSkipping token t, didn't match with says\u001b[0m\n",
      "\u001b[33mSkipping token arr, didn't match with says\u001b[0m\n",
      "\u001b[33mSkipping token in, didn't match with says\u001b[0m\n",
      "says -- says\n",
      "the -- the\n",
      "planet -- planet\n",
      "\u001b[33mSkipping token 's, didn't match with thriving\u001b[0m\n",
      "\u001b[33mSkipping token a, didn't match with thriving\u001b[0m\n",
      "\u001b[33mSkipping token freaking, didn't match with thriving\u001b[0m\n",
      "\u001b[33mSkipping token gem, didn't match with thriving\u001b[0m\n",
      "\u001b[33mSkipping token stone, didn't match with thriving\u001b[0m\n",
      "thriving -- th\n",
      "thriving -- ri\n",
      "thriving -- ving\n",
      "with -- with\n",
      "life -- life\n",
      "large -- large\n",
      "and -- and\n",
      "small -- small\n",
      "forms -- forms\n",
      "no -- no\n",
      "buildings -- buildings\n",
      "of -- of\n",
      "any -- any\n",
      "kind -- kind\n",
      "\u001b[33mSkipping token hydro, didn't match with metals\u001b[0m\n",
      "\u001b[33mSkipping token car, didn't match with metals\u001b[0m\n",
      "\u001b[33mSkipping token bons, didn't match with metals\u001b[0m\n",
      "metals -- metals\n",
      "and -- and\n",
      "\u001b[33mSkipping token a, didn't match with stable\u001b[0m\n",
      "stable -- stable\n",
      "atmosphere -- atmosphere\n",
      "he -- he\n",
      "\u001b[33mSkipping token 's, didn't match with says\u001b[0m\n",
      "\u001b[33mSkipping token not, didn't match with says\u001b[0m\n",
      "\u001b[33mSkipping token even, didn't match with says\u001b[0m\n",
      "\u001b[33mSkipping token bringing, didn't match with says\u001b[0m\n",
      "\u001b[33mSkipping token them, didn't match with says\u001b[0m\n",
      "\u001b[33mSkipping token back, didn't match with says\u001b[0m\n",
      "\u001b[33mSkipping token through, didn't match with says\u001b[0m\n",
      "\u001b[33mSkipping token for, didn't match with says\u001b[0m\n",
      "\u001b[33mSkipping token a, didn't match with says\u001b[0m\n",
      "\u001b[33mSkipping token face, didn't match with says\u001b[0m\n",
      "\u001b[33mSkipping token to, didn't match with says\u001b[0m\n",
      "\u001b[33mSkipping token face, didn't match with says\u001b[0m\n",
      "\u001b[33mSkipping token briefing, didn't match with says\u001b[0m\n",
      "\u001b[33mSkipping token he, didn't match with says\u001b[0m\n",
      "says -- says\n",
      "we -- we\n",
      "should -- should\n",
      "come -- come\n",
      "\u001b[33mSkipping token through, didn't match with now\u001b[0m\n",
      "now -- now\n",
      "immediately -- immediately\n",
      "before -- before\n",
      "the -- the\n",
      "probe -- probe\n",
      "is -- is\n",
      "reported -- reported\n",
      "late -- late\n",
      "harmon -- h\n",
      "harmon -- arm\n",
      "harmon -- on\n",
      "\u001b[33mSkipping token tell, didn't match with that\u001b[0m\n",
      "that -- that\n",
      "good -- good\n",
      "\u001b[33mSkipping token for, didn't match with nothing\u001b[0m\n",
      "nothing -- nothing\n",
      "son -- son\n",
      "of -- of\n",
      "\u001b[33mSkipping token a, didn't match with beta\u001b[0m\n",
      "beta -- beta\n",
      "\u001b[33mSkipping token to, didn't match with get\u001b[0m\n",
      "get -- get\n",
      "his -- his\n",
      "\u001b[33mSkipping token hind, didn't match with the\u001b[0m\n",
      "\u001b[33mSkipping token quarters, didn't match with the\u001b[0m\n",
      "\u001b[33mSkipping token through, didn't match with the\u001b[0m\n",
      "the -- the\n",
      "gate -- gate\n",
      "and -- and\n",
      "\u001b[33mSkipping token back, didn't match with here\u001b[0m\n",
      "here -- here\n",
      "now -- now\n",
      "or -- or\n",
      "\u001b[33mSkipping token i, didn't match with open\u001b[0m\n",
      "\u001b[33mSkipping token 'll, didn't match with open\u001b[0m\n",
      "open -- open\n",
      "fire -- fire\n",
      "on -- on\n",
      "him -- him\n",
      "when -- when\n",
      "we -- we\n",
      "come -- come\n",
      "\u001b[33mSkipping token back, didn't match with tara\u001b[0m\n",
      "\u001b[33mSkipping token through, didn't match with tara\u001b[0m\n",
      "tara -- tara\n",
      "\u001b[33mSkipping token f, didn't match with of\u001b[0m\n",
      "\u001b[33mSkipping token umed, didn't match with of\u001b[0m\n",
      "of -- of\n",
      "all -- all\n",
      "the -- the\n",
      "\u001b[33mSkipping token imper, didn't match with no\u001b[0m\n",
      "\u001b[33mSkipping token t, didn't match with no\u001b[0m\n",
      "\u001b[33mSkipping token inence, didn't match with no\u001b[0m\n",
      "\u001b[33mSkipping token t, didn't match with no\u001b[0m\n",
      "\u001b[33mSkipping token arr, didn't match with no\u001b[0m\n",
      "\u001b[33mSkipping token in, didn't match with no\u001b[0m\n",
      "no -- no\n",
      "doubt -- doubt\n",
      "with -- with\n",
      "orders -- orders\n",
      "from -- from\n",
      "mason -- mason\n",
      "was -- was\n",
      "questioning -- questioning\n",
      "her -- her\n",
      "command -- command\n",
      "decisions -- decisions\n",
      "\u001b[33mSkipping token that, didn't match with what\u001b[0m\n",
      "\u001b[33mSkipping token 's, didn't match with what\u001b[0m\n",
      "what -- what\n",
      "i -- i\n",
      "get -- get\n",
      "for -- for\n",
      "not -- not\n",
      "using -- using\n",
      "mercenaries -- mercenaries\n",
      "she -- she\n",
      "returned -- returned\n",
      "to -- to\n",
      "her -- her\n",
      "watch -- watch\n",
      "regarding -- regarding\n",
      "her -- her\n",
      "own -- own\n",
      "reflection -- reflection\n",
      "in -- in\n",
      "the -- the\n",
      "long -- long\n",
      "window -- window\n",
      "mahogany -- mah\n",
      "mahogany -- og\n",
      "mahogany -- any\n",
      "black -- black\n",
      "curly -- curly\n",
      "hair -- hair\n",
      "cut -- cut\n",
      "short -- short\n",
      "in -- in\n",
      "the -- the\n",
      "typical -- typical\n",
      "military -- military\n",
      "style -- style\n",
      "framed -- framed\n",
      "a -- a\n",
      "\u001b[33mSkipping token care, didn't match with dark\u001b[0m\n",
      "\u001b[33mSkipping token worn, didn't match with dark\u001b[0m\n",
      "dark -- dark\n",
      "skinned -- skinned\n",
      "face -- face\n",
      "it -- it\n",
      "was -- was\n",
      "not -- not\n",
      "an -- an\n",
      "attractive -- attractive\n",
      "face -- face\n",
      "right -- right\n",
      "now -- now\n",
      "her -- her\n",
      "ebony -- eb\n",
      "ebony -- ony\n",
      "eyes -- eyes\n",
      "\u001b[33mSkipping token shadow, didn't match with by\u001b[0m\n",
      "\u001b[33mSkipping token ed, didn't match with by\u001b[0m\n",
      "by -- by\n",
      "hours -- hours\n",
      "on -- on\n",
      "the -- the\n",
      "watch -- watch\n",
      "full -- full\n",
      "lips -- lips\n",
      "\u001b[33mSkipping token purs, didn't match with with\u001b[0m\n",
      "\u001b[33mSkipping token ed, didn't match with with\u001b[0m\n",
      "with -- with\n",
      "frustration -- frustration\n",
      "she -- she\n",
      "had -- had\n",
      "the -- the\n",
      "look -- look\n",
      "of -- of\n",
      "every -- every\n",
      "leader -- leader\n",
      "she -- she\n",
      "had -- had\n",
      "ever -- ever\n",
      "known -- known\n",
      "at -- at\n",
      "six -- six\n",
      "foot -- foot\n",
      "two -- two\n",
      "she -- she\n",
      "stood -- stood\n",
      "a -- a\n",
      "full -- full\n",
      "head -- head\n",
      "taller -- taller\n",
      "than -- than\n",
      "even -- even\n",
      "her -- her\n",
      "\u001b[33mSkipping token ar, didn't match with first\u001b[0m\n",
      "\u001b[33mSkipping token r, didn't match with first\u001b[0m\n",
      "\u001b[33mSkipping token all, didn't match with first\u001b[0m\n",
      "\u001b[33mSkipping token in, didn't match with first\u001b[0m\n",
      "first -- first\n",
      "officer -- officer\n",
      "her -- her\n",
      "\u001b[33mSkipping token gl, didn't match with expression\u001b[0m\n",
      "\u001b[33mSkipping token ower, didn't match with expression\u001b[0m\n",
      "\u001b[33mSkipping token ing, didn't match with expression\u001b[0m\n",
      "expression -- expression\n",
      "completed -- completed\n",
      "the -- the\n",
      "imposing -- imposing\n",
      "effect -- effect\n",
      "she -- she\n",
      "picked -- picked\n",
      "imaginary -- imaginary\n",
      "flecks -- fle\n",
      "flecks -- cks\n",
      "off -- off\n",
      "her -- her\n",
      "stark -- stark\n",
      "gray -- gray\n",
      "jumpsuit -- jumps\n",
      "jumpsuit -- uit\n",
      "and -- and\n",
      "snorted -- sn\n",
      "snorted -- orted\n",
      "you -- you\n",
      "look -- look\n",
      "like -- like\n",
      "hell -- hell\n",
      "tar -- tar\n",
      "\u001b[33mSkipping token don, didn't match with blow\u001b[0m\n",
      "\u001b[33mSkipping token 't, didn't match with blow\u001b[0m\n",
      "blow -- blow\n",
      "this -- this\n",
      "\u001b[33mSkipping token don, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token 't, didn't match with let\u001b[0m\n",
      "let -- let\n",
      "fatigue -- fatigue\n",
      "get -- get\n",
      "to -- to\n",
      "you -- you\n",
      "she -- she\n",
      "used -- used\n",
      "the -- the\n",
      "discretion -- discretion\n",
      "of -- of\n",
      "the -- the\n",
      "mirrored -- mirrored\n",
      "window -- window\n",
      "to -- to\n",
      "\u001b[33mSkipping token secret, didn't match with to\u001b[0m\n",
      "\u001b[33mSkipping token ively, didn't match with to\u001b[0m\n",
      "\u001b[33mSkipping token survey, didn't match with to\u001b[0m\n",
      "\u001b[33mSkipping token her, didn't match with to\u001b[0m\n",
      "\u001b[33mSkipping token crew, didn't match with to\u001b[0m\n",
      "\u001b[33mSkipping token harmon, didn't match with to\u001b[0m\n",
      "\u001b[33mSkipping token was, didn't match with to\u001b[0m\n",
      "\u001b[33mSkipping token an, didn't match with to\u001b[0m\n",
      "\u001b[33mSkipping token excellent, didn't match with to\u001b[0m\n",
      "\u001b[33mSkipping token addition, didn't match with to\u001b[0m\n",
      "to -- to\n",
      "\u001b[33mSkipping token the, didn't match with defy\u001b[0m\n",
      "\u001b[33mSkipping token bridge, didn't match with defy\u001b[0m\n",
      "\u001b[33mSkipping token team, didn't match with defy\u001b[0m\n",
      "\u001b[33mSkipping token but, didn't match with defy\u001b[0m\n",
      "\u001b[33mSkipping token was, didn't match with defy\u001b[0m\n",
      "\u001b[33mSkipping token she, didn't match with defy\u001b[0m\n",
      "\u001b[33mSkipping token open, didn't match with defy\u001b[0m\n",
      "\u001b[33mSkipping token minded, didn't match with defy\u001b[0m\n",
      "\u001b[33mSkipping token enough, didn't match with defy\u001b[0m\n",
      "\u001b[33mSkipping token to, didn't match with defy\u001b[0m\n",
      "defy -- defy\n",
      "united -- united\n",
      "earth -- earth\n",
      "command -- command\n",
      "and -- and\n",
      "sever -- sever\n",
      "all -- all\n",
      "ties -- ties\n",
      "from -- from\n",
      "earth -- earth\n",
      "she -- she\n",
      "watched -- watched\n",
      "the -- the\n",
      "psi -- psi\n",
      "bob -- bob\n",
      "her -- her\n",
      "head -- head\n",
      "unaware -- unaware\n",
      "of -- of\n",
      "the -- the\n",
      "rest -- rest\n",
      "of -- of\n",
      "the -- the\n",
      "bridge -- bridge\n",
      "shaking -- shaking\n",
      "blonde -- blonde\n",
      "curls -- curls\n",
      "as -- as\n",
      "she -- she\n",
      "\u001b[33mSkipping token chat, didn't match with away\u001b[0m\n",
      "\u001b[33mSkipping token tered, didn't match with away\u001b[0m\n",
      "away -- away\n",
      "with -- with\n",
      "the -- the\n",
      "approaching -- approaching\n",
      "psi -- psi\n",
      "relay -- relay\n",
      "on -- on\n",
      "\u001b[33mSkipping token mason, didn't match with ship\u001b[0m\n",
      "\u001b[33mSkipping token 's, didn't match with ship\u001b[0m\n",
      "ship -- ship\n",
      "she -- she\n",
      "seemed -- seemed\n",
      "so -- so\n",
      "\u001b[33mSkipping token depth, didn't match with self\u001b[0m\n",
      "\u001b[33mSkipping token less, didn't match with self\u001b[0m\n",
      "\u001b[33mSkipping token âģķ, didn't match with self\u001b[0m\n",
      "self -- self\n",
      "conscious -- conscious\n",
      "and -- and\n",
      "shallow -- shallow\n",
      "on -- on\n",
      "the -- the\n",
      "outside -- outside\n",
      "but -- but\n",
      "having -- having\n",
      "that -- that\n",
      "incredible -- incredible\n",
      "gift -- gift\n",
      "there -- there\n",
      "had -- had\n",
      "to -- to\n",
      "be -- be\n",
      "more -- more\n",
      "to -- to\n",
      "her -- her\n",
      "well -- well\n",
      "if -- if\n",
      "she -- she\n",
      "\u001b[33mSkipping token didn, didn't match with pan\u001b[0m\n",
      "\u001b[33mSkipping token 't, didn't match with pan\u001b[0m\n",
      "pan -- pan\n",
      "out -- out\n",
      "\u001b[33mSkipping token she, didn't match with be\u001b[0m\n",
      "\u001b[33mSkipping token 'd, didn't match with be\u001b[0m\n",
      "be -- be\n",
      "terminated -- terminated\n",
      "any -- any\n",
      "crew -- crew\n",
      "member -- member\n",
      "that -- that\n",
      "would -- would\n",
      "jeopardize -- jeopard\n",
      "jeopardize -- ize\n",
      "the -- the\n",
      "project -- project\n",
      "was -- was\n",
      "meat -- meat\n",
      "it -- it\n",
      "would -- would\n",
      "be -- be\n",
      "a -- a\n",
      "shame -- shame\n",
      "to -- to\n",
      "lose -- lose\n",
      "that -- that\n",
      "talent -- talent\n",
      "though -- though\n",
      "\u001b[33mSkipping token rak, didn't match with her\u001b[0m\n",
      "\u001b[33mSkipping token al, didn't match with her\u001b[0m\n",
      "her -- her\n",
      "first -- first\n",
      "officer -- officer\n",
      "was -- was\n",
      "staring -- staring\n",
      "\u001b[33mSkipping token p, didn't match with at\u001b[0m\n",
      "\u001b[33mSkipping token ensive, didn't match with at\u001b[0m\n",
      "\u001b[33mSkipping token ly, didn't match with at\u001b[0m\n",
      "at -- at\n",
      "his -- his\n",
      "panel -- panel\n",
      "he -- he\n",
      "was -- was\n",
      "what -- what\n",
      "this -- this\n",
      "was -- was\n",
      "all -- all\n",
      "about -- about\n",
      "she -- she\n",
      "watched -- watched\n",
      "his -- his\n",
      "graceful -- graceful\n",
      "fingers -- fingers\n",
      "ending -- ending\n",
      "in -- in\n",
      "thick -- thick\n",
      "black -- black\n",
      "claws -- claws\n",
      "tap -- tap\n",
      "out -- out\n",
      "calculations -- calculations\n",
      "on -- on\n",
      "the -- the\n",
      "panel -- panel\n",
      "his -- his\n",
      "pointed -- pointed\n",
      "ears -- ears\n",
      "\u001b[33mSkipping token sw, didn't match with back\u001b[0m\n",
      "\u001b[33mSkipping token ive, didn't match with back\u001b[0m\n",
      "\u001b[33mSkipping token led, didn't match with back\u001b[0m\n",
      "back -- back\n",
      "and -- and\n",
      "forth -- forth\n",
      "catching -- catching\n",
      "every -- every\n",
      "sound -- sound\n",
      "from -- from\n",
      "the -- the\n",
      "bridge -- bridge\n",
      "while -- while\n",
      "his -- his\n",
      "long -- long\n",
      "tail -- tail\n",
      "swished -- sw\n",
      "swished -- ished\n",
      "to -- to\n",
      "the -- the\n",
      "rhythm -- rhythm\n",
      "of -- of\n",
      "his -- his\n",
      "thoughts -- thoughts\n",
      "only -- only\n",
      "those -- those\n",
      "of -- of\n",
      "the -- the\n",
      "\u001b[33mSkipping token ar, didn't match with insurrection\u001b[0m\n",
      "\u001b[33mSkipping token r, didn't match with insurrection\u001b[0m\n",
      "\u001b[33mSkipping token all, didn't match with insurrection\u001b[0m\n",
      "insurrection -- in\n",
      "\u001b[31mResidual FA el surrection not covered by token ins. Drop transaction.\u001b[0m\n",
      "    story  story_uid  sound_id  kind                     meg_file      start  \\\n",
      "464   lw1        0.0       2.0  word  A0167_MASC_1_16Mar17_01.con  64.080000   \n",
      "465   lw1        0.0       2.0  word  A0167_MASC_1_16Mar17_01.con  65.840000   \n",
      "466   lw1        0.0       2.0  word  A0167_MASC_1_16Mar17_01.con  66.200000   \n",
      "467   lw1        0.0       2.0  word  A0167_MASC_1_16Mar17_01.con  66.480000   \n",
      "468   lw1        0.0       2.0  word  A0167_MASC_1_16Mar17_01.con  66.660000   \n",
      "469   lw1        0.0       2.0  word  A0167_MASC_1_16Mar17_01.con  67.299999   \n",
      "470   lw1        0.0       2.0  word  A0167_MASC_1_16Mar17_01.con  68.110000   \n",
      "471   lw1        0.0       2.0  word  A0167_MASC_1_16Mar17_01.con  68.380000   \n",
      "472   lw1        0.0       2.0  word  A0167_MASC_1_16Mar17_01.con  68.759999   \n",
      "473   lw1        0.0       2.0  word  A0167_MASC_1_16Mar17_01.con  68.940000   \n",
      "\n",
      "                       sound          word  sequence_id condition  word_index  \\\n",
      "464  stimuli/audio/lw1_2.wav      thoughts         38.0  sentence        23.0   \n",
      "465  stimuli/audio/lw1_2.wav          Only         39.0  sentence         0.0   \n",
      "466  stimuli/audio/lw1_2.wav         those         39.0  sentence         1.0   \n",
      "467  stimuli/audio/lw1_2.wav            of         39.0  sentence         2.0   \n",
      "468  stimuli/audio/lw1_2.wav           the         39.0  sentence         3.0   \n",
      "469  stimuli/audio/lw1_2.wav  Insurrection         39.0  sentence         5.0   \n",
      "470  stimuli/audio/lw1_2.wav         inner         39.0  sentence         6.0   \n",
      "471  stimuli/audio/lw1_2.wav          team         39.0  sentence         7.0   \n",
      "472  stimuli/audio/lw1_2.wav          knew         39.0  sentence         8.0   \n",
      "473  stimuli/audio/lw1_2.wav            he         39.0  sentence         9.0   \n",
      "\n",
      "     speech_rate    voice  pronounced    onset  duration  value  sample  \n",
      "464        145.0  Allison         1.0  274.128      0.59   2468  274128  \n",
      "465        145.0  Allison         1.0  275.888      0.36   2469  275888  \n",
      "466        145.0  Allison         1.0  276.248      0.28   2470  276248  \n",
      "467        145.0  Allison         1.0  276.528      0.18   2471  276528  \n",
      "468        145.0  Allison         1.0  276.708      0.18   2472  276708  \n",
      "469        145.0  Allison         1.0  277.348      0.81   2473  277348  \n",
      "470        145.0  Allison         1.0  278.158      0.27   2474  278158  \n",
      "471        145.0  Allison         1.0  278.428      0.38   2475  278428  \n",
      "472        145.0  Allison         1.0  278.808      0.17   2476  278808  \n",
      "473        145.0  Allison         1.0  278.988      0.14   2477  278988  \n",
      "['Ġthe', 'ĠAr', 'r', 'all', 'in', 'ĠIns', 'urrection', 'Ġinner', 'Ġteam', 'Ġknew']\n",
      "30\n",
      "\u001b[33mSkipping token in, didn't match with insurrection\u001b[0m\n",
      "insurrection -- ins\n",
      "insurrection -- urrection\n",
      "inner -- inner\n",
      "team -- team\n",
      "knew -- knew\n",
      "he -- he\n",
      "was -- was\n",
      "no -- no\n",
      "common -- common\n",
      "beta -- beta\n",
      "furry -- furry\n",
      "his -- his\n",
      "silken -- sil\n",
      "silken -- ken\n",
      "fur -- fur\n",
      "which -- which\n",
      "would -- would\n",
      "be -- be\n",
      "\u001b[33mSkipping token t, didn't match with golden\u001b[0m\n",
      "\u001b[33mSkipping token awn, didn't match with golden\u001b[0m\n",
      "\u001b[33mSkipping token y, didn't match with golden\u001b[0m\n",
      "golden -- golden\n",
      "and -- and\n",
      "striped -- striped\n",
      "with -- with\n",
      "jet -- jet\n",
      "black -- black\n",
      "bands -- bands\n",
      "was -- was\n",
      "dyed -- dyed\n",
      "perfectly -- perfectly\n",
      "to -- to\n",
      "a -- a\n",
      "pure -- pure\n",
      "black -- black\n",
      "and -- and\n",
      "his -- his\n",
      "mane -- man\n",
      "mane -- e\n",
      "trimmed -- trimmed\n",
      "and -- and\n",
      "thinned -- thin\n",
      "thinned -- ned\n",
      "as -- as\n",
      "to -- to\n",
      "be -- be\n",
      "indistinguishable -- indistinguishable\n",
      "from -- from\n",
      "the -- the\n",
      "rest -- rest\n",
      "of -- of\n",
      "his -- his\n",
      "coat -- coat\n",
      "his -- his\n",
      "eyes -- eyes\n",
      "had -- had\n",
      "been -- been\n",
      "treated -- treated\n",
      "and -- and\n",
      "darkened -- darkened\n",
      "to -- to\n",
      "a -- a\n",
      "rich -- rich\n",
      "purple -- purple\n",
      "to -- to\n",
      "disguise -- disguise\n",
      "the -- the\n",
      "brilliant -- brilliant\n",
      "golden -- golden\n",
      "yellow -- yellow\n",
      "color -- color\n",
      "that -- that\n",
      "would -- would\n",
      "mark -- mark\n",
      "him -- him\n",
      "as -- as\n",
      "an -- an\n",
      "alpha -- alpha\n",
      "\u001b[33mSkipping token ar, didn't match with and\u001b[0m\n",
      "\u001b[33mSkipping token r, didn't match with and\u001b[0m\n",
      "\u001b[33mSkipping token all, didn't match with and\u001b[0m\n",
      "\u001b[33mSkipping token in, didn't match with and\u001b[0m\n",
      "and -- and\n",
      "leader -- leader\n",
      "of -- of\n",
      "his -- his\n",
      "\u001b[33mSkipping token hive, didn't match with right\u001b[0m\n",
      "right -- right\n",
      "now -- now\n",
      "he -- he\n",
      "looked -- looked\n",
      "like -- like\n",
      "an -- an\n",
      "overgrown -- over\n",
      "overgrown -- grown\n",
      "wolf -- wolf\n",
      "\u001b[33mSkipping token who, didn't match with learned\u001b[0m\n",
      "\u001b[33mSkipping token 'd, didn't match with learned\u001b[0m\n",
      "learned -- learned\n",
      "touch -- touch\n",
      "typing -- ty\n",
      "typing -- ping\n",
      "the -- the\n",
      "scout -- scout\n",
      "ship -- ship\n",
      "\u001b[33mSkipping token re, didn't match with from\u001b[0m\n",
      "\u001b[33mSkipping token emer, didn't match with from\u001b[0m\n",
      "\u001b[33mSkipping token ged, didn't match with from\u001b[0m\n",
      "from -- from\n",
      "the -- the\n",
      "hole -- hole\n",
      "\u001b[33mSkipping token âģķ, didn't match with a\u001b[0m\n",
      "a -- a\n",
      "brilliant -- brilliant\n",
      "speck -- spe\n",
      "speck -- ck\n",
      "emerging -- emerging\n",
      "from -- from\n",
      "a -- a\n",
      "sphere -- sphere\n",
      "of -- of\n",
      "velvety -- vel\n",
      "velvety -- ve\n",
      "velvety -- ty\n",
      "blackness -- black\n",
      "blackness -- ness\n",
      "its -- its\n",
      "hail -- hail\n",
      "\u001b[33mSkipping token crack, didn't match with across\u001b[0m\n",
      "\u001b[33mSkipping token led, didn't match with across\u001b[0m\n",
      "across -- across\n",
      "the -- the\n",
      "\u001b[33mSkipping token comm, didn't match with and\u001b[0m\n",
      "and -- and\n",
      "tara -- tara\n",
      "spun -- spun\n",
      "to -- to\n",
      "retake -- retake\n",
      "her -- her\n",
      "seat -- seat\n",
      "at -- at\n",
      "the -- the\n",
      "helm -- helm\n",
      "launch -- launch\n",
      "the -- the\n",
      "second -- second\n",
      "probe -- probe\n",
      "\u001b[33mSkipping token won, didn't match with central\u001b[0m\n",
      "\u001b[33mSkipping token 't, didn't match with central\u001b[0m\n",
      "central -- central\n",
      "be -- be\n",
      "crushed -- crushed\n",
      "to -- to\n",
      "learn -- learn\n",
      "that -- that\n",
      "another -- another\n",
      "gateway -- gateway\n",
      "has -- has\n",
      "yielded -- yielded\n",
      "little -- little\n",
      "more -- more\n",
      "than -- than\n",
      "a -- a\n",
      "class -- class\n",
      "f -- f\n",
      "planet -- planet\n",
      "and -- and\n",
      "a -- a\n",
      "white -- white\n",
      "dwarf -- dwarf\n",
      "system -- system\n",
      "level -- level\n",
      "\u001b[33mSkipping token 1, didn't match with and\u001b[0m\n",
      "and -- and\n",
      "\u001b[33mSkipping token 2, didn't match with and\u001b[0m\n",
      "\u001b[33mSkipping token staffers, didn't match with and\u001b[0m\n",
      "\u001b[33mSkipping token should, didn't match with and\u001b[0m\n",
      "\u001b[33mSkipping token prepare, didn't match with and\u001b[0m\n",
      "\u001b[33mSkipping token to, didn't match with and\u001b[0m\n",
      "\u001b[33mSkipping token be, didn't match with and\u001b[0m\n",
      "\u001b[33mSkipping token briefed, didn't match with and\u001b[0m\n",
      "and -- and\n",
      "\u001b[33mSkipping token move, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token out, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token this, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token sounds, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token like, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token it, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token 's, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token the, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token one, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token the, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token distinctive, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token wh, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token uffle, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token of, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token pleasure, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token r, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token ipp, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token led, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token through, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token the, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token bet, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token as, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token on, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token the, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token bridge, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token and, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token rak, didn't match with let\u001b[0m\n",
      "\u001b[33mSkipping token al, didn't match with let\u001b[0m\n",
      "let -- let\n",
      "loose -- loose\n",
      "a -- a\n",
      "small -- small\n",
      "growl -- grow\n",
      "growl -- l\n",
      "as -- as\n",
      "if -- if\n",
      "\u001b[33mSkipping token to, didn't match with caution\u001b[0m\n",
      "caution -- caution\n",
      "his -- his\n",
      "charges -- charges\n",
      "against -- against\n",
      "false -- false\n",
      "hope -- hope\n",
      "\u001b[33mSkipping token they, didn't match with twenty\u001b[0m\n",
      "\u001b[33mSkipping token 'd, didn't match with twenty\u001b[0m\n",
      "\u001b[33mSkipping token sc, didn't match with twenty\u001b[0m\n",
      "\u001b[33mSkipping token outed, didn't match with twenty\u001b[0m\n",
      "twenty -- twenty\n",
      "seven -- seven\n",
      "gates -- gates\n",
      "so -- so\n",
      "far -- far\n",
      "and -- and\n",
      "none -- none\n",
      "had -- had\n",
      "turned -- turned\n",
      "up -- up\n",
      "anything -- anything\n",
      "worth -- worth\n",
      "\u001b[33mSkipping token the, didn't match with time\u001b[0m\n",
      "\u001b[33mSkipping token ins, didn't match with time\u001b[0m\n",
      "\u001b[33mSkipping token urrection, didn't match with time\u001b[0m\n",
      "\u001b[33mSkipping token 's, didn't match with time\u001b[0m\n",
      "time -- time\n",
      "tara -- tara\n",
      "would -- would\n",
      "not -- not\n",
      "let -- let\n",
      "giddy -- g\n",
      "giddy -- iddy\n",
      "hopes -- hopes\n",
      "drag -- drag\n",
      "them -- them\n",
      "onto -- onto\n",
      "a -- a\n",
      "rock -- rock\n",
      "that -- that\n",
      "would -- would\n",
      "spell -- spell\n",
      "\u001b[33mSkipping token the, didn't match with end\u001b[0m\n",
      "end -- end\n",
      "for -- for\n",
      "\u001b[33mSkipping token the, didn't match with project\u001b[0m\n",
      "project -- project\n",
      "and -- and\n",
      "\u001b[33mSkipping token the, didn't match with species\u001b[0m\n",
      "\u001b[33mSkipping token ar, didn't match with species\u001b[0m\n",
      "\u001b[33mSkipping token r, didn't match with species\u001b[0m\n",
      "\u001b[33mSkipping token all, didn't match with species\u001b[0m\n",
      "\u001b[33mSkipping token in, didn't match with species\u001b[0m\n",
      "species -- species\n"
     ]
    }
   ],
   "source": [
    "alignment = Aligner(tokens, words_df, max_skip_patience=30)()\n",
    "alignment = pd.DataFrame(alignment, columns=[\"word_idx\", \"token_idx\", \"flags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a3d40da-c132-401a-b7bf-37ecd4d7cbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_idx</th>\n",
       "      <th>story</th>\n",
       "      <th>story_uid</th>\n",
       "      <th>sound_id</th>\n",
       "      <th>kind</th>\n",
       "      <th>meg_file</th>\n",
       "      <th>start</th>\n",
       "      <th>sound</th>\n",
       "      <th>word</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>condition</th>\n",
       "      <th>word_index</th>\n",
       "      <th>speech_rate</th>\n",
       "      <th>voice</th>\n",
       "      <th>pronounced</th>\n",
       "      <th>onset</th>\n",
       "      <th>duration</th>\n",
       "      <th>value</th>\n",
       "      <th>sample</th>\n",
       "      <th>token_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>stimuli/audio/lw1_0.wav</td>\n",
       "      <td>Tara</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.506</td>\n",
       "      <td>0.30</td>\n",
       "      <td>697</td>\n",
       "      <td>23506</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>stimuli/audio/lw1_0.wav</td>\n",
       "      <td>Tara</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.506</td>\n",
       "      <td>0.30</td>\n",
       "      <td>697</td>\n",
       "      <td>23506</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>stimuli/audio/lw1_0.wav</td>\n",
       "      <td>stood</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>1.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.816</td>\n",
       "      <td>0.24</td>\n",
       "      <td>698</td>\n",
       "      <td>23816</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>stimuli/audio/lw1_0.wav</td>\n",
       "      <td>stock</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>2.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.056</td>\n",
       "      <td>0.37</td>\n",
       "      <td>699</td>\n",
       "      <td>24056</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>stimuli/audio/lw1_0.wav</td>\n",
       "      <td>still</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>3.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.586</td>\n",
       "      <td>0.40</td>\n",
       "      <td>700</td>\n",
       "      <td>24586</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>663</td>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>50.070000</td>\n",
       "      <td>stimuli/audio/lw1_3.wav</td>\n",
       "      <td>end</td>\n",
       "      <td>52.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>15.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>361.097</td>\n",
       "      <td>0.17</td>\n",
       "      <td>3119</td>\n",
       "      <td>361097</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>664</td>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>50.250000</td>\n",
       "      <td>stimuli/audio/lw1_3.wav</td>\n",
       "      <td>for</td>\n",
       "      <td>52.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>16.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>361.277</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3120</td>\n",
       "      <td>361277</td>\n",
       "      <td>919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>665</td>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>50.460000</td>\n",
       "      <td>stimuli/audio/lw1_3.wav</td>\n",
       "      <td>project</td>\n",
       "      <td>52.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>18.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>361.487</td>\n",
       "      <td>0.58</td>\n",
       "      <td>3121</td>\n",
       "      <td>361487</td>\n",
       "      <td>921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>666</td>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>51.179999</td>\n",
       "      <td>stimuli/audio/lw1_3.wav</td>\n",
       "      <td>and</td>\n",
       "      <td>52.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>19.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>362.207</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3122</td>\n",
       "      <td>362207</td>\n",
       "      <td>923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>667</td>\n",
       "      <td>lw1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>word</td>\n",
       "      <td>A0167_MASC_1_16Mar17_01.con</td>\n",
       "      <td>51.790000</td>\n",
       "      <td>stimuli/audio/lw1_3.wav</td>\n",
       "      <td>species</td>\n",
       "      <td>52.0</td>\n",
       "      <td>sentence</td>\n",
       "      <td>22.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>362.817</td>\n",
       "      <td>0.34</td>\n",
       "      <td>3123</td>\n",
       "      <td>362817</td>\n",
       "      <td>929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>639 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     word_idx story  story_uid  sound_id  kind                     meg_file  \\\n",
       "0           0   lw1        0.0       0.0  word  A0167_MASC_1_16Mar17_01.con   \n",
       "1           0   lw1        0.0       0.0  word  A0167_MASC_1_16Mar17_01.con   \n",
       "2           1   lw1        0.0       0.0  word  A0167_MASC_1_16Mar17_01.con   \n",
       "3           2   lw1        0.0       0.0  word  A0167_MASC_1_16Mar17_01.con   \n",
       "4           3   lw1        0.0       0.0  word  A0167_MASC_1_16Mar17_01.con   \n",
       "..        ...   ...        ...       ...   ...                          ...   \n",
       "634       663   lw1        0.0       3.0  word  A0167_MASC_1_16Mar17_01.con   \n",
       "635       664   lw1        0.0       3.0  word  A0167_MASC_1_16Mar17_01.con   \n",
       "636       665   lw1        0.0       3.0  word  A0167_MASC_1_16Mar17_01.con   \n",
       "637       666   lw1        0.0       3.0  word  A0167_MASC_1_16Mar17_01.con   \n",
       "638       667   lw1        0.0       3.0  word  A0167_MASC_1_16Mar17_01.con   \n",
       "\n",
       "         start                    sound     word  sequence_id condition  \\\n",
       "0     0.000000  stimuli/audio/lw1_0.wav     Tara          0.0  sentence   \n",
       "1     0.000000  stimuli/audio/lw1_0.wav     Tara          0.0  sentence   \n",
       "2     0.310000  stimuli/audio/lw1_0.wav    stood          0.0  sentence   \n",
       "3     0.550000  stimuli/audio/lw1_0.wav    stock          0.0  sentence   \n",
       "4     1.080000  stimuli/audio/lw1_0.wav    still          0.0  sentence   \n",
       "..         ...                      ...      ...          ...       ...   \n",
       "634  50.070000  stimuli/audio/lw1_3.wav      end         52.0  sentence   \n",
       "635  50.250000  stimuli/audio/lw1_3.wav      for         52.0  sentence   \n",
       "636  50.460000  stimuli/audio/lw1_3.wav  project         52.0  sentence   \n",
       "637  51.179999  stimuli/audio/lw1_3.wav      and         52.0  sentence   \n",
       "638  51.790000  stimuli/audio/lw1_3.wav  species         52.0  sentence   \n",
       "\n",
       "     word_index  speech_rate    voice  pronounced    onset  duration  value  \\\n",
       "0           0.0        205.0  Allison         1.0   23.506      0.30    697   \n",
       "1           0.0        205.0  Allison         1.0   23.506      0.30    697   \n",
       "2           1.0        205.0  Allison         1.0   23.816      0.24    698   \n",
       "3           2.0        205.0  Allison         1.0   24.056      0.37    699   \n",
       "4           3.0        205.0  Allison         1.0   24.586      0.40    700   \n",
       "..          ...          ...      ...         ...      ...       ...    ...   \n",
       "634        15.0        205.0  Allison         1.0  361.097      0.17   3119   \n",
       "635        16.0        205.0  Allison         1.0  361.277      0.14   3120   \n",
       "636        18.0        205.0  Allison         1.0  361.487      0.58   3121   \n",
       "637        19.0        205.0  Allison         1.0  362.207      0.15   3122   \n",
       "638        22.0        205.0  Allison         1.0  362.817      0.34   3123   \n",
       "\n",
       "     sample  token_idx  \n",
       "0     23506          0  \n",
       "1     23506          1  \n",
       "2     23816          2  \n",
       "3     24056          3  \n",
       "4     24586          5  \n",
       "..      ...        ...  \n",
       "634  361097        918  \n",
       "635  361277        919  \n",
       "636  361487        921  \n",
       "637  362207        923  \n",
       "638  362817        929  \n",
       "\n",
       "[639 rows x 20 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NB inner join -- will duplicate word rows if there are multiple corresponding tokens.\n",
    "words_df = pd.merge(words_df.rename_axis(\"word_idx\").reset_index(),\n",
    "                    alignment.drop(columns=[\"flags\"]),\n",
    "                    on=\"word_idx\")\n",
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "901f4ff5-62f6-4469-a3ee-0aedebdab25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asof merge: join phonemes to most recent corresponding word onset. NB the merged token idx will be the last subword token of the corresponding word\n",
    "phonemes_df = pd.merge_asof(phonemes_df, words_df[[\"onset\", \"duration\", \"token_idx\", \"word_idx\"]].assign(end_word=words_df.onset + words_df.duration),\n",
    "                            on=\"onset\", direction=\"backward\", suffixes=(\"\", \"_word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bfd9d9e-de50-4b35-bb9a-7f73e8f80415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 255 phoneme instances from words not matched with tokens\n",
      "2207 phonemes remain.\n"
     ]
    }
   ],
   "source": [
    "# But this means phonemes of words unassociated with elements of words_df will\n",
    "# be matched with onsets of the most recent word that is indeed in words_df.\n",
    "# That's no good. Find these and drop.\n",
    "phonemes_to_drop = phonemes_df.onset >= phonemes_df.end_word\n",
    "print(f\"Dropping {phonemes_to_drop.sum()} phoneme instances from words not matched with tokens\")\n",
    "\n",
    "phonemes_df = phonemes_df[~phonemes_to_drop]\n",
    "print(f\"{len(phonemes_df)} phonemes remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab144593-1724-4c7c-8c15-027e738b7f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_idx\n",
       "253     1\n",
       "602     1\n",
       "43      1\n",
       "212     1\n",
       "561     1\n",
       "       ..\n",
       "359    10\n",
       "88     12\n",
       "438    12\n",
       "69     12\n",
       "508    16\n",
       "Length: 613, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify there is a reasonable word length distribution\n",
    "phonemes_df.groupby(\"word_idx\").size().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4da050dc-d134-4f7c-80df-396769beec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words with missing phoneme data.\n",
    "words_df = words_df[words_df.word_idx.isin(set(phonemes_df.word_idx))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26febb7a-677e-4638-b629-275aeaa10681",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(words_df.word_idx) == set(phonemes_df.word_idx), \\\n",
    "    \"Word and phoneme level annotations should cover the same set of word IDs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "714cc189-b514-4303-a92f-8523fe998b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df.to_csv(Path(output_dir) / \"word.csv\")\n",
    "phonemes_df.to_csv(Path(output_dir) / \"phoneme.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c19b1-9d34-4f4d-bf62-5d026a90316c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
