{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10df71a-1a18-4cc8-9463-5522488bdf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "L = logging.getLogger(__name__)\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c779d9-57dd-43e3-af30-a3c771bfa620",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(str(Path(\".\").resolve().parent.parent))\n",
    "\n",
    "from berp.datasets import BerpDataset, NestedBerpDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "17f23ec3-6058-44e8-92b5-5b6dc07794a3",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "story_name = \"old-man-and-the-sea\"\n",
    "subject = 15\n",
    "run = 1\n",
    "\n",
    "aligned_words = \"word.csv\"\n",
    "aligned_phonemes = \"phoneme.csv\"\n",
    "# aligned_words = f\"../../workflow/heilbron2022/data/aligned/distilgpt2/{story_name}/word.csv\"\n",
    "# aligned_phonemes = f\"../../workflow/heilbron2022/data/aligned/distilgpt2/{story_name}/phoneme.csv\"\n",
    "\n",
    "stimulus = \"old-man-and-the-sea/run1.pkl\"\n",
    "# stimulus = f\"../../workflow/heilbron2022/data/stimulus/distilgpt2/n1000/{story_name}/run{run}.pkl\"\n",
    "\n",
    "run_Xy = f\"../../workflow/heilbron2022/data/run_Xy/{story_name}/sub{subject}/run{run}.h5\"\n",
    "# run_Xy = \"/home/jgauthie/om2/others/heilbron2022/notebooks/exported/Xy/words_proba/sub1/run2.h5\"\n",
    "\n",
    "# This argument controls the meaning of the output `word_onsets` property. Non-default values may result\n",
    "# in filtering of the dataset (words and constituent phonemes) in order to render the analysis interpretable.\n",
    "# By default word onset obviously means word onset: the onset of the first phoneme in the acoustic stimulus.\n",
    "#\n",
    "# NOTE that this does not affect any time series onset features -- only the features accessed at the Berp\n",
    "# representational level (the `word_onsets` property of a `BerpDataset`).\n",
    "#\n",
    "# Other options:\n",
    "# - `\"second_syllable\"`: words have onset at their second syllable. This enables a control analysis to evaluate\n",
    "#   how meaningful \"word onset\" is in itself, versus any meaningful point within a word. The dataset will be\n",
    "#   filtered such that only multisyllabic words are present. TODO what happens to phonemes prior to second syllable onset?\n",
    "word_onset_event = \"second-syllable\"\n",
    "\n",
    "target_sample_rate = 128\n",
    "output_path = f\"{story_name}.sub{subject}.run{run}.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bed2d4fc-a3b8-4c11-8727-0f2f496abc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if word_onset_event not in [\"word\", \"second-syllable\"]:\n",
    "    raise ValueError(f\"unknown value for `word_onset_event`: {word_onset_event}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3282c557-9417-4f5f-987c-f817a5674a3d",
   "metadata": {},
   "source": [
    "## Load and process natural language stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0eeef0-6754-4621-9596-dd88fd706031",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(stimulus).open(\"rb\") as f:\n",
    "    story_stim = pickle.load(f)\n",
    "    \n",
    "assert story_stim.name == f\"{story_name}/run{run}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae48b57-9914-45d6-a2ec-b72ec57e684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable onset features are simply a variable onset intercept,\n",
    "# word features and word surprisals.\n",
    "X_variable = torch.concat(\n",
    "    [torch.ones_like(story_stim.word_surprisals).unsqueeze(1),\n",
    "     story_stim.word_features,\n",
    "     story_stim.word_surprisals.unsqueeze(1)],\n",
    "    dim=1)\n",
    "variable_feature_names = [\"recognition_onset\"] + story_stim.word_feature_names + [\"word_surprisal\"]\n",
    "\n",
    "assert X_variable.shape[1] == len(variable_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6324d161-2cb4-4790-bbd3-f42bbc570cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recognition_onset', 'word_frequency', 'word_surprisal']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1034c5c6-4c60-476a-9741-76b25c3f0052",
   "metadata": {},
   "source": [
    "## Load aligned word/phoneme presentation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb2dcb33-2431-40bf-88e3-0ae790c678bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>token_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HE</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>WAS</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AN</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>OLD</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.41</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MAN</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.79</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>572</td>\n",
       "      <td>INTO</td>\n",
       "      <td>175.78</td>\n",
       "      <td>175.98</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>573</td>\n",
       "      <td>STRIPS</td>\n",
       "      <td>175.98</td>\n",
       "      <td>176.37</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>574</td>\n",
       "      <td>FOR</td>\n",
       "      <td>176.37</td>\n",
       "      <td>176.48</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>575</td>\n",
       "      <td>SALTING</td>\n",
       "      <td>176.48</td>\n",
       "      <td>177.01</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>575</td>\n",
       "      <td>SALTING</td>\n",
       "      <td>176.48</td>\n",
       "      <td>177.01</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     word_idx     word   onset  offset  token_idx\n",
       "0           0       HE    0.76    0.88          0\n",
       "1           1      WAS    0.88    1.04          1\n",
       "2           2       AN    1.04    1.12          2\n",
       "3           3      OLD    1.12    1.41          3\n",
       "4           4      MAN    1.41    1.79          4\n",
       "..        ...      ...     ...     ...        ...\n",
       "608       572     INTO  175.78  175.98        682\n",
       "609       573   STRIPS  175.98  176.37        683\n",
       "610       574      FOR  176.37  176.48        684\n",
       "611       575  SALTING  176.48  177.01        685\n",
       "612       575  SALTING  176.48  177.01        686\n",
       "\n",
       "[613 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_aligned_df = pd.read_csv(aligned_words, index_col=[0, 1]).loc[run].reset_index()\n",
    "phoneme_aligned_df = pd.read_csv(aligned_phonemes, index_col=[0, 1]).loc[run].reset_index()\n",
    "word_aligned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6552fb-a10f-4286-a060-40a95db23257",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check for words missing in stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "989b45fe-c516-45cd-b84e-9abb6943b4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 419}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Not all words in the presentation will be retained in the stimulus: words which\n",
    "# ended up at the start of an input time series to the model were dropped, because\n",
    "# the model didn't have those values as targets.\n",
    "#\n",
    "# By the above logic, the missing words will likely be distributed roughly evenly\n",
    "# throughout the stimulus.\n",
    "words_missing_in_stimulus = set(word_aligned_df.word_idx) - set(story_stim.word_ids.numpy())\n",
    "\n",
    "plt.figure(figsize=(15, 1))\n",
    "plt.hlines(0, 0, story_stim.word_ids.max())\n",
    "plt.plot(list(words_missing_in_stimulus), np.zeros(len(words_missing_in_stimulus)), 'ro', ms=8, mfc='r')\n",
    "plt.axis('off')\n",
    "plt.title(\"Words missing in stimulus representation\")\n",
    "plt.tight_layout()\n",
    "\n",
    "words_missing_in_stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b10e29cc-d411-4899-b7a6-9226af994c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0 == len(set(story_stim.word_ids.numpy()) - set(word_aligned_df.word_idx)), \\\n",
    "    \"Stim words are present which are missing from the aligned data!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e1a68-9271-445d-9c5d-c731df836df2",
   "metadata": {},
   "source": [
    "### Check for phonemes missing in stimulus\n",
    "\n",
    "We're guiding the check by `story_stim.word_ids`, so we won't run into any of the missing words we already uncovered in `word_ids` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d083247f-f61a-4335-8a1a-d36322ea3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phoneme annotations according to aligned df\n",
    "aligned_phoneme_lookup = phoneme_aligned_df.groupby(\"word_idx\").phoneme.apply(tuple).to_dict()\n",
    "\n",
    "for i, word_id in enumerate(story_stim.word_ids.numpy()):\n",
    "    stim_phons = story_stim.get_candidate_strs(i, 1)[0]\n",
    "    aligned_phons = aligned_phoneme_lookup[word_id]\n",
    "\n",
    "    assert aligned_phons == stim_phons, \\\n",
    "        f\"{word_id}: {aligned_phons} != {stim_phons}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c3e68-268f-449d-8335-1eb36bc7be8b",
   "metadata": {},
   "source": [
    "## Load design matrix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2ca6f4b-73f1-4348-a976-eaa6c33ef3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(run_Xy) as f:\n",
    "    assert subject == f.attrs[\"subject\"]\n",
    "    assert run == f.attrs[\"run\"]\n",
    "    \n",
    "    X = f[\"X\"][()]\n",
    "    y = f[\"y\"][()].T\n",
    "    \n",
    "    sensor_names = f.attrs[\"ch_names\"].tolist()\n",
    "    ts_feature_names = f.attrs[\"feature_names\"].tolist()\n",
    "    sfreq = f.attrs[\"sfreq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d69879d-fdff-4d51-85b2-5f6846a0a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape[0] == y.shape[0]\n",
    "assert X.shape[1] == len(ts_feature_names)\n",
    "assert y.shape[1] == len(sensor_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5cd19d",
   "metadata": {},
   "source": [
    "### Add intercept feature to time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a359c1f-6638-4ef3-a445-4d9380d0c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "ts_feature_names = ['intercept'] + ts_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573308d4-d7f7-402a-b435-a1ca0be543b7",
   "metadata": {},
   "source": [
    "### Remove surprisal+frequency data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "965bc4e9-2d79-47e8-b70c-928b92011718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intercept',\n",
       " 'all_words_onset',\n",
       " 'all_phons_onset',\n",
       " 'all_phons_pitch',\n",
       " 'all_phons_env_var',\n",
       " 'all_phons_surprisals',\n",
       " 'all_phons_sg1',\n",
       " 'all_phons_sg2',\n",
       " 'all_phons_sg3',\n",
       " 'all_phons_sg4',\n",
       " 'all_phons_sg5',\n",
       " 'all_phons_sg6',\n",
       " 'all_phons_sg7',\n",
       " 'all_phons_sg8']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_ts_features = [\"all_words_lexical_surprise\", \"all_words_unigram_surprise\"]\n",
    "keep_ts_feature_idxs = [i for i, name in enumerate(ts_feature_names) if name not in drop_ts_features]\n",
    "\n",
    "X = X[:, keep_ts_feature_idxs]\n",
    "ts_feature_names = [name for i, name in enumerate(ts_feature_names) if i in keep_ts_feature_idxs]\n",
    "ts_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd3e5b-3a50-41c7-87c5-bd39e011ef2c",
   "metadata": {},
   "source": [
    "## Check agreement of onset features with alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929e7d00-e1a4-4f4e-9242-6ce16229c3be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Word onset\n",
    "\n",
    "Verify that the word onsets as represented in the design matrix agree with the word onset data as given in the aligned annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2be90bf-aca9-4726-8223-d13ff68bbe54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7578125, 0.8828125, 1.0390625, 1.1171875, 1.40625  , 1.8671875,\n",
       "       2.078125 , 2.40625  , 2.7734375, 2.8828125])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_onset_idx = ts_feature_names.index(\"all_words_onset\")\n",
    "X_onset_times = X[:, word_onset_idx].nonzero()[0] / sfreq\n",
    "X_onset_times[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26ea3877-01ef-474c-b2aa-7ba66ee3ac5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76, 0.88, 1.04, 1.12, 1.41, 1.87, 2.08, 2.41, 2.77, 2.88])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_onset_times = word_aligned_df.groupby(\"word_idx\").onset.min()\n",
    "aligned_onset_times.values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0850046b-cf9c-4efb-a97b-67d7cb045c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x108 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 1.5))\n",
    "plt.hlines(0, 0, X_onset_times.max())\n",
    "plt.plot(X_onset_times, np.zeros(len(X_onset_times)), 'ro', ms=8, mfc='r')\n",
    "plt.plot(aligned_onset_times, np.ones(len(aligned_onset_times)), 'go', ms=8)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d217f7a5-2c87-420f-8d5b-52d18f30b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words that appear in the Heilbron annotation but don't make any sense,\n",
    "# and which we want to ignore.\n",
    "FORSAKEN_WORDS = [\n",
    "    # run_idx, word_onset, first_phoneme_onset, num_phonemes\n",
    "    (3, 22.77, 22.77, 4),  # \"boat\" transcribed phonemically as /buks/\n",
    "    (16, 102.51, 102.52, 7),  # \"dubbing\" transcribed as /dklʌbɪŋ/\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19f004d2-1fa6-4724-b017-b42171f30526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB aligned annotation only has precision to 2 places, so we'll have to round.\n",
    "X_onset_times_corrected = X_onset_times.round(2)\n",
    "drop_times = np.array([time for exc_run, time, _, _ in FORSAKEN_WORDS if exc_run == run])\n",
    "X_onset_times_corrected = X_onset_times_corrected[~np.in1d(X_onset_times_corrected, drop_times)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fde73eb4-3497-48d6-9da0-41c6efa0559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_onset_times_corrected), len(aligned_onset_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c9cf902-854f-4df4-9d1c-0cacf2cc12a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(X_onset_times_corrected) - set(aligned_onset_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e83a91bd-7ce3-44dd-8036-f4b6cb27c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_onset_times_corrected) == len(aligned_onset_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a2399bc-0a67-4682-a9c0-cf94eb894055",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(X_onset_times_corrected, aligned_onset_times, atol=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed0e0f6f-8e28-42ba-a515-6c3c935cbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find overlap\n",
    "# array1 = X_onset_times\n",
    "# array2 = word_aligned_df.onset.unique()\n",
    "# import itertools\n",
    "# from tqdm.auto import trange\n",
    "# matches = []\n",
    "# no_match = []\n",
    "# for i0 in trange(array1.shape[0]):\n",
    "#     for i1 in range(array2.shape[0]):\n",
    "#         if np.isclose(array1[i0], array2[i1], atol=1e-1):\n",
    "#             matches.append((i0, i1))\n",
    "#             break\n",
    "#     else:\n",
    "#         no_match.append(i0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "147b4494-3d65-4e69-8fb1-05926132e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(no_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "418241c6-62fb-45f0-847d-5ffc64b7f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3a13ab2-221e-4aed-bc23-6c3957bcf228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15, 2))\n",
    "# plt.hlines(0, 0, X_onset_times.max())\n",
    "# mask = np.zeros(len(X_onset_times), dtype=bool)\n",
    "# mask[no_match] = True\n",
    "# plt.plot(X_onset_times[mask], np.zeros(mask.sum()), 'ro', ms=8, mfc='r', alpha=0.3)\n",
    "# plt.plot(X_onset_times[~mask], np.zeros((~mask).sum()), 'ro', ms=8, mfc='g', alpha=0.3)\n",
    "# # plt.plot(word_aligned_df.onset.values, np.ones(len(word_aligned_df)), 'go', ms=8)\n",
    "# plt.axis('off')\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289da941-787c-46ef-a4c7-c8271af807f0",
   "metadata": {},
   "source": [
    "### Phoneme onset\n",
    "\n",
    "We'll do the same thing now for phoneme onsets, making sure there are no discrepancies between our understanding of phoneme onset and the assumptions going into the design matrix production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13412c57-c9f8-4b87-9bd8-00df751f2ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.765625 , 0.8125   , 0.8828125, 0.9609375, 0.9921875, 1.0390625,\n",
       "       1.09375  , 1.125    , 1.1796875, 1.3046875])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phon_onset_idx = ts_feature_names.index(\"all_phons_onset\")\n",
    "X_phon_onset_times = X[:, phon_onset_idx].nonzero()[0] / sfreq\n",
    "X_phon_onset_times[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4af7aef-6af2-4b3e-b596-eca6b0558060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77, 0.81, 0.88, 0.96, 0.99, 1.04, 1.09, 1.12, 1.18, 1.3 ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_phon_onset_times = phoneme_aligned_df.onset\n",
    "aligned_phon_onset_times.values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87ac360c-1b62-4702-b4f6-bc7c4ac06848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x108 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 1.5))\n",
    "plt.hlines(0, 0, X_phon_onset_times.max())\n",
    "plt.plot(X_phon_onset_times, np.zeros(len(X_phon_onset_times)), 'ro', ms=8, mfc='r')\n",
    "plt.plot(aligned_phon_onset_times, np.ones(len(aligned_phon_onset_times)), 'go', ms=8)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7266532c-e8a7-4dbb-b908-9c87fb697ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_phon_onset_times_corrected = X_phon_onset_times.round(2)\n",
    "# Drop onset times for phonemes from `FORSAKEN_WORDS`\n",
    "for exc_run, _, first_phon_onset_time, num_phonemes in FORSAKEN_WORDS:\n",
    "    if exc_run == run:\n",
    "        drop_start_idx = np.where(X_phon_onset_times_corrected == first_phon_onset_time)[0][0]\n",
    "        X_phon_onset_times_corrected = np.concatenate([X_phon_onset_times_corrected[:drop_start_idx],\n",
    "                                                       X_phon_onset_times_corrected[drop_start_idx + num_phonemes:]])\n",
    "X_onset_times_corrected = X_onset_times_corrected[~np.in1d(X_onset_times_corrected, drop_times)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1a3e6f8-4cef-4969-aad4-3ce1ab06d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_phon_onset_times_corrected), len(aligned_phon_onset_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56e597bd-38fe-476b-bb77-0ff3d7997c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(X_phon_onset_times_corrected) - set(aligned_phon_onset_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d1c9663-cf1e-49d0-9811-13b7c7881093",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(X_phon_onset_times_corrected, aligned_phon_onset_times, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fe2be-bed9-44c1-a341-136a929d4df9",
   "metadata": {},
   "source": [
    "## Load signal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1397bb9e-a151-458c-8f66-2066b30a7788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178.5625"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_time, max_time = 0, len(X) / sfreq\n",
    "max_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8baf303e-3a30-4760-a47f-525f4f1feb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check compatibility with alignment data.\n",
    "assert min_time <= word_aligned_df.onset.min()\n",
    "assert min_time <= phoneme_aligned_df.onset.min()\n",
    "assert max_time >= word_aligned_df.offset.max()\n",
    "assert max_time >= phoneme_aligned_df.offset.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b173f2-892c-4afa-97c4-7db41be597dd",
   "metadata": {},
   "source": [
    "## Replace onset features\n",
    "\n",
    "Even though we already checked that the two annotations agree, we're going to insert our own aligned to the sample rate, just to make sure the setup is absolutely perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d393433-6208-455c-ba1a-72c29113c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_aligned_df[\"onset\"] = (word_aligned_df[\"onset\"] * target_sample_rate).round() / target_sample_rate\n",
    "word_aligned_df[\"offset\"] = (word_aligned_df[\"offset\"] * target_sample_rate).round() / target_sample_rate\n",
    "phoneme_aligned_df[\"onset\"] = (phoneme_aligned_df[\"onset\"] * target_sample_rate).round() / target_sample_rate\n",
    "phoneme_aligned_df[\"offset\"] = (phoneme_aligned_df[\"offset\"] * target_sample_rate).round() / target_sample_rate\n",
    "phoneme_aligned_df[\"offset_word\"] = (phoneme_aligned_df[\"offset_word\"] * target_sample_rate).round() / target_sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d52f8b54-ac39-45cb-93c6-6c407b1dffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_onset_samples = (word_aligned_df[\"onset\"] * target_sample_rate).astype(int)\n",
    "phoneme_onset_samples = (phoneme_aligned_df[\"onset\"] * target_sample_rate).astype(int)\n",
    "\n",
    "# Drop words+phonemes which extend past the end of the time series.\n",
    "word_onset_samples = word_onset_samples[word_onset_samples < len(X)]\n",
    "phoneme_onset_samples = phoneme_onset_samples[phoneme_onset_samples < len(X)]\n",
    "\n",
    "word_onset_features = np.zeros((len(X), 1))\n",
    "phoneme_onset_features = np.zeros((len(X), 1))\n",
    "word_onset_features[word_onset_samples] = 1\n",
    "phoneme_onset_features[phoneme_onset_samples] = 1\n",
    "\n",
    "X = np.concatenate((X, word_onset_features, phoneme_onset_features), axis=1)\n",
    "ts_feature_names += [\"word_onset\", \"phoneme_onset\"]\n",
    "assert X.shape[1] == len(ts_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6a9a04b-760e-4f75-be6c-1bc52a3d6cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No feature c_words_onset\n"
     ]
    }
   ],
   "source": [
    "# Remove Heilbron onset data.\n",
    "remove_onset_features = [\"all_words_onset\", \"c_words_onset\", \"all_phons_onset\"]\n",
    "for feat in remove_onset_features:\n",
    "    try:\n",
    "        feat_idx = ts_feature_names.index(feat)\n",
    "    except ValueError:\n",
    "        L.warning(f\"No feature {feat}\")\n",
    "    else:\n",
    "        X = np.delete(X, feat_idx, axis=1)\n",
    "        ts_feature_names = ts_feature_names[:feat_idx] + ts_feature_names[feat_idx + 1:]\n",
    "        \n",
    "assert X.shape[1] == len(ts_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94853519-f107-41eb-8a61-72ffc3111071",
   "metadata": {},
   "source": [
    "## Add phoneme-level features to time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec7f1630-0e87-40eb-b43b-b5965d8b1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_ts = torch.zeros((X.shape[0], len(story_stim.phoneme_feature_names)))\n",
    "\n",
    "phoneme_lookup_df = phoneme_aligned_df.set_index(\"word_idx\")\n",
    "for word_idx, phoneme_features in zip(story_stim.word_ids, story_stim.phoneme_features):\n",
    "    onsets = np.array(list(phoneme_lookup_df.loc[[word_idx.item()], \"onset\"]))\n",
    "    onset_samps = torch.tensor(onsets * target_sample_rate).long()\n",
    "    phoneme_ts[onset_samps, :] = phoneme_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b73b6b6c-f4a8-4506-8834-7c746554f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat([torch.tensor(X), phoneme_ts], axis=1)\n",
    "ts_feature_names += story_stim.phoneme_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a390af-4501-4259-957a-cae23e7c5584",
   "metadata": {},
   "source": [
    "## Produce BerpDataset representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a08684e1-6c49-4423-827d-d93e713c1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute phoneme onsets relative to word onset.\n",
    "phoneme_onsets = phoneme_aligned_df.groupby(\"word_idx\") \\\n",
    "    .apply(lambda xs: list(xs.onset - xs.onset.min())).to_dict()\n",
    "phoneme_onsets = [torch.tensor(phoneme_onsets.get(word_id.item(), [np.nan]))\n",
    "                  for word_id in story_stim.word_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ece3d43a-0108-45c3-a8bb-cff6272e26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_offsets = word_aligned_df.groupby(\"word_idx\").offset.max().to_dict()\n",
    "word_offsets = torch.tensor([word_offsets.get(word_id.item(), np.nan)\n",
    "                             for word_id in story_stim.word_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e21035b0-a161-40c2-b93c-1a534f3d38b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_words = torch.arange(len(story_stim.word_ids))\n",
    "\n",
    "# Produce onset information from canonical aligned data.\n",
    "word_onsets = word_aligned_df.groupby(\"word_idx\").onset.min().to_dict()\n",
    "word_onsets = torch.tensor([word_onsets.get(word_id.item(), np.nan)\n",
    "                            for word_id in story_stim.word_ids])\n",
    "\n",
    "if word_onset_event == \"second-syllable\":\n",
    "    keep_words, second_syllable_onsets = [], []\n",
    "    for idx, (syllable_onsets_i, phoneme_onsets_i) in enumerate(zip(story_stim.word_events[\"syllable_onset\"], phoneme_onsets)):\n",
    "        if len(syllable_onsets_i) < 2:\n",
    "            continue\n",
    "        \n",
    "        keep_words.append(idx)\n",
    "        second_syllable_onsets.append(phoneme_onsets_i[syllable_onsets_i[1]])\n",
    "\n",
    "    keep_words = torch.tensor(keep_words)\n",
    "    \n",
    "    word_onsets = word_onsets[keep_words] + torch.tensor(second_syllable_onsets)\n",
    "    # Filter everything else, too.\n",
    "    phoneme_onsets = [phoneme_onsets[i] - second_syllable_time for i, second_syllable_time in zip(keep_words, second_syllable_onsets)]\n",
    "    word_offsets = word_offsets[keep_words]\n",
    "    X_variable = X_variable[keep_words]\n",
    "elif word_onset_event == \"word\":\n",
    "    pass\n",
    "else:\n",
    "    raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b9f95a4-461a-41c8-b905-2d85627f0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_phonemes = max(len(onsets) for onsets in phoneme_onsets)\n",
    "\n",
    "# Sanity check: max_num_phonemes as computed from aligned data should\n",
    "# match that produced earlier by the natural language stimulus processor\n",
    "assert max_num_phonemes == story_stim.max_n_phonemes, \\\n",
    "    \"%d %d\" % (max_num_phonemes, story_stim.max_n_phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bacdcad4-7106-495d-a484-37b6633a33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad phoneme onset information\n",
    "phoneme_onsets = torch.stack([\n",
    "    pad(onsets, (0, max_num_phonemes - len(onsets)), value=0.)\n",
    "    if len(onsets) < max_num_phonemes\n",
    "    else onsets\n",
    "    for onsets in phoneme_onsets\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "75c95267-5d48-4270-ac19-59c8876ae342",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BerpDataset(\n",
    "    name=f\"{story_name}/sub{subject}/run{run}\",\n",
    "    stimulus_name=story_stim.name,\n",
    "    sample_rate=int(sfreq),\n",
    "    \n",
    "    phonemes=story_stim.phonemes,\n",
    "    \n",
    "    retain_stim_word_ids=keep_words,\n",
    "    word_onsets=word_onsets,\n",
    "    word_offsets=word_offsets,\n",
    "    phoneme_onsets=phoneme_onsets,\n",
    "    \n",
    "    X_ts=X,\n",
    "    ts_feature_names=ts_feature_names,\n",
    "    \n",
    "    X_variable=X_variable,\n",
    "    variable_feature_names=variable_feature_names,\n",
    "    \n",
    "    Y=y,\n",
    "    sensor_names=sensor_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bfc08d54-edaa-4e43-9178-41b0251a2b95",
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'berp.datasets.base.BerpDataset'>: it's not the same object as berp.datasets.base.BerpDataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [71], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Path(output_path)\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'berp.datasets.base.BerpDataset'>: it's not the same object as berp.datasets.base.BerpDataset"
     ]
    }
   ],
   "source": [
    "with Path(output_path).open(\"wb\") as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1dd637-deb0-4033-89c1-b5910a5fcab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad3ae7957b733d35bf078bb149f2f6d2bad023390430cb2ebf73db3aa5db82f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
