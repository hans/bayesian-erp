{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10df71a-1a18-4cc8-9463-5522488bdf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "L = logging.getLogger(__name__)\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c779d9-57dd-43e3-af30-a3c771bfa620",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(str(Path(\".\").resolve().parent.parent))\n",
    "\n",
    "from berp.datasets import BerpDataset, NestedBerpDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f23ec3-6058-44e8-92b5-5b6dc07794a3",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "story_name = \"old-man-and-the-sea\"\n",
    "subject = 1\n",
    "run = 2\n",
    "\n",
    "# aligned_words = \"word.csv\"\n",
    "# aligned_phonemes = \"phoneme.csv\"\n",
    "aligned_words = f\"../../workflow/heilbron2022/data/aligned/distilgpt2/{story_name}/word.csv\"\n",
    "aligned_phonemes = f\"../../workflow/heilbron2022/data/aligned/distilgpt2/{story_name}/phoneme.csv\"\n",
    "\n",
    "# stimulus = \"old-man-and-the-sea/run2.pkl\"\n",
    "stimulus = f\"../../workflow/heilbron2022/data/stimulus/distilgpt2/{story_name}/run{run}.pkl\"\n",
    "\n",
    "run_Xy = f\"../../workflow/heilbron2022/data/run_Xy/{story_name}/sub{subject}/run{run}.h5\"\n",
    "# run_Xy = \"/home/jgauthie/om2/others/heilbron2022/notebooks/exported/Xy/words_proba/sub1/run2.h5\"\n",
    "\n",
    "target_sample_rate = 128\n",
    "output_path = f\"{story_name}.sub{subject}.run{run}.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3282c557-9417-4f5f-987c-f817a5674a3d",
   "metadata": {},
   "source": [
    "## Load and process natural language stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb0eeef0-6754-4621-9596-dd88fd706031",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(stimulus).open(\"rb\") as f:\n",
    "    story_stim = pickle.load(f)\n",
    "    \n",
    "assert story_stim.name == f\"{story_name}/run{run}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae48b57-9914-45d6-a2ec-b72ec57e684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable onset features are simply a variable onset intercept,\n",
    "# word features and word surprisals.\n",
    "X_variable = torch.concat(\n",
    "    [torch.ones_like(story_stim.word_surprisals).unsqueeze(1),\n",
    "     story_stim.word_features,\n",
    "     story_stim.word_surprisals.unsqueeze(1)],\n",
    "    dim=1)\n",
    "variable_feature_names = [\"recognition_onset\"] + story_stim.word_feature_names + [\"word_surprisal\"]\n",
    "\n",
    "assert X_variable.shape[1] == len(variable_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6324d161-2cb4-4790-bbd3-f42bbc570cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recognition_onset', 'word_frequency', 'word_surprisal']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1034c5c6-4c60-476a-9741-76b25c3f0052",
   "metadata": {},
   "source": [
    "## Load aligned word/phoneme presentation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb2dcb33-2431-40bf-88e3-0ae790c678bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>token_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>WHEN</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.31</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>THE</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.37</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>WIND</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.61</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>WAS</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.77</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>IN</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.87</td>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>541</td>\n",
       "      <td>AND</td>\n",
       "      <td>179.26</td>\n",
       "      <td>179.35</td>\n",
       "      <td>1364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>542</td>\n",
       "      <td>YOUR</td>\n",
       "      <td>179.35</td>\n",
       "      <td>179.56</td>\n",
       "      <td>1365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>543</td>\n",
       "      <td>EYES</td>\n",
       "      <td>179.56</td>\n",
       "      <td>179.77</td>\n",
       "      <td>1366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>544</td>\n",
       "      <td>ARE</td>\n",
       "      <td>179.77</td>\n",
       "      <td>179.83</td>\n",
       "      <td>1367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>545</td>\n",
       "      <td>GOOD</td>\n",
       "      <td>179.83</td>\n",
       "      <td>180.16</td>\n",
       "      <td>1368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>572 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     word_idx  word   onset  offset  token_idx\n",
       "0           0  WHEN    1.17    1.31        688\n",
       "1           1   THE    1.31    1.37        689\n",
       "2           2  WIND    1.37    1.61        690\n",
       "3           3   WAS    1.61    1.77        691\n",
       "4           4    IN    1.77    1.87        692\n",
       "..        ...   ...     ...     ...        ...\n",
       "567       541   AND  179.26  179.35       1364\n",
       "568       542  YOUR  179.35  179.56       1365\n",
       "569       543  EYES  179.56  179.77       1366\n",
       "570       544   ARE  179.77  179.83       1367\n",
       "571       545  GOOD  179.83  180.16       1368\n",
       "\n",
       "[572 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_aligned_df = pd.read_csv(aligned_words, index_col=[0, 1]).loc[run].reset_index()\n",
    "phoneme_aligned_df = pd.read_csv(aligned_phonemes, index_col=[0, 1]).loc[run].reset_index()\n",
    "word_aligned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6552fb-a10f-4286-a060-40a95db23257",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check for words missing in stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "989b45fe-c516-45cd-b84e-9abb6943b4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{16,\n",
       " 46,\n",
       " 68,\n",
       " 116,\n",
       " 141,\n",
       " 170,\n",
       " 199,\n",
       " 227,\n",
       " 278,\n",
       " 301,\n",
       " 326,\n",
       " 349,\n",
       " 370,\n",
       " 399,\n",
       " 451,\n",
       " 503,\n",
       " 526}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Not all words in the presentation will be retained in the stimulus: words which\n",
    "# ended up at the start of an input time series to the model were dropped, because\n",
    "# the model didn't have those values as targets.\n",
    "#\n",
    "# By the above logic, the missing words will likely be distributed roughly evenly\n",
    "# throughout the stimulus.\n",
    "words_missing_in_stimulus = set(word_aligned_df.word_idx) - set(story_stim.word_ids.numpy())\n",
    "\n",
    "plt.figure(figsize=(15, 1))\n",
    "plt.hlines(0, 0, story_stim.word_ids.max())\n",
    "plt.plot(list(words_missing_in_stimulus), np.zeros(len(words_missing_in_stimulus)), 'ro', ms=8, mfc='r')\n",
    "plt.axis('off')\n",
    "plt.title(\"Words missing in stimulus representation\")\n",
    "plt.tight_layout()\n",
    "\n",
    "words_missing_in_stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b10e29cc-d411-4899-b7a6-9226af994c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0 == len(set(story_stim.word_ids.numpy()) - set(word_aligned_df.word_idx)), \\\n",
    "    \"Stim words are present which are missing from the aligned data!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c3e68-268f-449d-8335-1eb36bc7be8b",
   "metadata": {},
   "source": [
    "## Load design matrix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ca6f4b-73f1-4348-a976-eaa6c33ef3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(run_Xy) as f:\n",
    "    assert subject == f.attrs[\"subject\"]\n",
    "    assert run == f.attrs[\"run\"]\n",
    "    \n",
    "    X = f[\"X\"][()]\n",
    "    y = f[\"y\"][()].T\n",
    "    \n",
    "    sensor_names = f.attrs[\"ch_names\"].tolist()\n",
    "    ts_feature_names = f.attrs[\"feature_names\"].tolist()\n",
    "    sfreq = f.attrs[\"sfreq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d69879d-fdff-4d51-85b2-5f6846a0a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape[0] == y.shape[0]\n",
    "assert X.shape[1] == len(ts_feature_names)\n",
    "assert y.shape[1] == len(sensor_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5cd19d",
   "metadata": {},
   "source": [
    "### Add intercept feature to time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a359c1f-6638-4ef3-a445-4d9380d0c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "ts_feature_names = ['intercept'] + ts_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573308d4-d7f7-402a-b435-a1ca0be543b7",
   "metadata": {},
   "source": [
    "### Remove surprisal+frequency data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "965bc4e9-2d79-47e8-b70c-928b92011718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intercept',\n",
       " 'all_words_onset',\n",
       " 'c_words_onset',\n",
       " 'c_words_contextual_dissimilarity',\n",
       " 'all_phons_pitch',\n",
       " 'all_phons_env_var',\n",
       " 'all_phons_sg1',\n",
       " 'all_phons_sg2',\n",
       " 'all_phons_sg3',\n",
       " 'all_phons_sg4',\n",
       " 'all_phons_sg5',\n",
       " 'all_phons_sg6',\n",
       " 'all_phons_sg7',\n",
       " 'all_phons_sg8']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_ts_features = [\"all_words_lexical_surprise\", \"all_words_unigram_surprise\"]\n",
    "keep_ts_feature_idxs = [i for i, name in enumerate(ts_feature_names) if name not in drop_ts_features]\n",
    "\n",
    "X = X[:, keep_ts_feature_idxs]\n",
    "ts_feature_names = [name for i, name in enumerate(ts_feature_names) if i in keep_ts_feature_idxs]\n",
    "ts_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929e7d00-e1a4-4f4e-9242-6ce16229c3be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SKIP: Align onset data\n",
    "\n",
    "(There are many mismatches between the onset data given in the design matrix and the raw onset data from the Heilbron stimulus. This is due to some internal logic of the rERP design matrix prep in Heilbron that I don't understand. Instead of trying to understand and harmonize, I just decided to redo the onset computations instead. See next section.)\n",
    "\n",
    "The design matrix is arranged such that the first word onset is $t=0$. Update the alignment dataframes to reflect this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2be90bf-aca9-4726-8223-d13ff68bbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_onset_idx = ts_feature_names.index(\"all_words_onset\")\n",
    "# X_onset_times = X[:, word_onset_idx].nonzero()[0] / sfreq\n",
    "# X_onset_times[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "796f1d1a-2ae5-448f-842a-b4b3b488f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift = word_aligned_df.onset.min()\n",
    "# word_aligned_df[\"onset\"] -= shift\n",
    "# word_aligned_df[\"offset\"] -= shift\n",
    "# phoneme_aligned_df[\"onset\"] -= shift\n",
    "# phoneme_aligned_df[\"offset\"] -= shift\n",
    "# phoneme_aligned_df[\"offset_word\"] -= shift\n",
    "\n",
    "# # TODO check consistency\n",
    "\n",
    "# # TODO check: every word onset should have a phoneme with corresponding onset;\n",
    "# # every word offset should have a phoneme with corresponding offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0850046b-cf9c-4efb-a97b-67d7cb045c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15, 2))\n",
    "# plt.hlines(0, 0, X_onset_times.max())\n",
    "# plt.plot(X_onset_times, np.zeros(len(X_onset_times)), 'ro', ms=8, mfc='r')\n",
    "# plt.plot(word_aligned_df.onset.values, np.ones(len(word_aligned_df)), 'go', ms=8)\n",
    "# plt.axis('off')\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed0e0f6f-8e28-42ba-a515-6c3c935cbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find overlap\n",
    "# array1 = X_onset_times\n",
    "# array2 = word_aligned_df.onset.unique()\n",
    "# import itertools\n",
    "# from tqdm.auto import trange\n",
    "# matches = []\n",
    "# no_match = []\n",
    "# for i0 in trange(array1.shape[0]):\n",
    "#     for i1 in range(array2.shape[0]):\n",
    "#         if np.isclose(array1[i0], array2[i1], atol=1e-1):\n",
    "#             matches.append((i0, i1))\n",
    "#             break\n",
    "#     else:\n",
    "#         no_match.append(i0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "147b4494-3d65-4e69-8fb1-05926132e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(no_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "418241c6-62fb-45f0-847d-5ffc64b7f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fab8b6db-1d55-4a05-869e-3e4196cf2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_match[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfef865f-2eeb-456b-8faf-cbeba3533a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_onset_times[40:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d8927b1-b729-45ec-99fc-b25ef03c1109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_onset_times[no_match[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3a13ab2-221e-4aed-bc23-6c3957bcf228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15, 2))\n",
    "# plt.hlines(0, 0, X_onset_times.max())\n",
    "# mask = np.zeros(len(X_onset_times), dtype=bool)\n",
    "# mask[no_match] = True\n",
    "# plt.plot(X_onset_times[mask], np.zeros(mask.sum()), 'ro', ms=8, mfc='r', alpha=0.3)\n",
    "# plt.plot(X_onset_times[~mask], np.zeros((~mask).sum()), 'ro', ms=8, mfc='g', alpha=0.3)\n",
    "# # plt.plot(word_aligned_df.onset.values, np.ones(len(word_aligned_df)), 'go', ms=8)\n",
    "# plt.axis('off')\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be935eb3-73da-4679-b206-5b3e198e92a8",
   "metadata": {},
   "source": [
    "#### TODO: investigate evident misalignment here\n",
    "\n",
    "Looks like there's a misalignment that starts with the onset of a new sentence \"Santiago\" and may trigger a massive misalignment for rest of run.\n",
    "\n",
    "For now we'll put up with this incongruity in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "def3cace-55c6-40eb-917a-1ae3ca6966d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_aligned_df.iloc[40:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9fe3588-08c4-49da-b38f-a316e6bb33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_onset_times), len(word_aligned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350d4d4-9197-4d2e-820b-7c1bb89c7ba4",
   "metadata": {},
   "source": [
    "### Shift aligned word/phoneme data\n",
    "\n",
    "Design matrix is shifted such that the time series begins exactly at the first word onset. Shift word / phoneme onsets accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ff487c8-a777-4a59-a633-fdf03541c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = word_aligned_df.onset.min()\n",
    "word_aligned_df[\"onset\"] -= shift\n",
    "word_aligned_df[\"offset\"] -= shift\n",
    "phoneme_aligned_df[\"onset\"] -= shift\n",
    "phoneme_aligned_df[\"offset\"] -= shift\n",
    "phoneme_aligned_df[\"offset_word\"] -= shift\n",
    "\n",
    "# TODO check consistency\n",
    "\n",
    "# TODO check: every word onset should have a phoneme with corresponding onset;\n",
    "# every word offset should have a phoneme with corresponding offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fe2be-bed9-44c1-a341-136a929d4df9",
   "metadata": {},
   "source": [
    "## Load signal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1397bb9e-a151-458c-8f66-2066b30a7788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163.7265625"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_time, max_time = 0, len(X) / sfreq\n",
    "max_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8baf303e-3a30-4760-a47f-525f4f1feb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45, 130 chopped words/phonemes. Dropping.\n"
     ]
    }
   ],
   "source": [
    "# Check compatibility with alignment data.\n",
    "assert min_time <= word_aligned_df.onset.min()\n",
    "assert min_time <= phoneme_aligned_df.onset.min()\n",
    "\n",
    "# NB looks like the last sentence-ish of each run is truncated from the signal\n",
    "# data. TODO double check this with authors?\n",
    "# assert max_time >= word_aligned_df.offset.max()\n",
    "# assert max_time >= phoneme_aligned_df.offset.max()\n",
    "chopped_words = word_aligned_df.offset >= max_time\n",
    "chopped_phonemes = phoneme_aligned_df.offset >= max_time\n",
    "print(f\"{chopped_words.sum()}, {chopped_phonemes.sum()} chopped words/phonemes. Dropping.\")\n",
    "\n",
    "word_aligned_df = word_aligned_df.loc[~chopped_words]\n",
    "phoneme_aligned_df = phoneme_aligned_df.loc[~chopped_phonemes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51e54531-3928-4b5c-b65d-5811e327305a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>token_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>498</td>\n",
       "      <td>TO</td>\n",
       "      <td>161.71</td>\n",
       "      <td>161.80</td>\n",
       "      <td>1305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>499</td>\n",
       "      <td>COME</td>\n",
       "      <td>161.80</td>\n",
       "      <td>161.98</td>\n",
       "      <td>1306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>500</td>\n",
       "      <td>OUT</td>\n",
       "      <td>161.98</td>\n",
       "      <td>162.14</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>501</td>\n",
       "      <td>AFTER</td>\n",
       "      <td>162.14</td>\n",
       "      <td>162.37</td>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>502</td>\n",
       "      <td>DOLPHIN</td>\n",
       "      <td>162.37</td>\n",
       "      <td>162.85</td>\n",
       "      <td>1309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word_idx     word   onset  offset  token_idx\n",
       "522       498       TO  161.71  161.80       1305\n",
       "523       499     COME  161.80  161.98       1306\n",
       "524       500      OUT  161.98  162.14       1307\n",
       "525       501    AFTER  162.14  162.37       1308\n",
       "526       502  DOLPHIN  162.37  162.85       1309"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_aligned_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b12999f2-5d40-4580-89aa-1e8993247215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We may have residual phonemes without a corresponding word now. Remove those.\n",
    "orphaned_phonemes = ~phoneme_aligned_df.word_idx.isin(word_aligned_df.word_idx)\n",
    "if orphaned_phonemes.any():\n",
    "    print(f\"Also dropping {orphaned_phonemes.sum()} orphaned phonemes.\")\n",
    "    phoneme_aligned_df = phoneme_aligned_df.loc[~orphaned_phonemes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec525b5b-44ca-474e-928c-f3c79494091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check consistency.\n",
    "assert set(word_aligned_df.word_idx) == set(phoneme_aligned_df.word_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b173f2-892c-4afa-97c4-7db41be597dd",
   "metadata": {},
   "source": [
    "## Replace onset features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d393433-6208-455c-ba1a-72c29113c5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47749/3278703401.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_aligned_df[\"onset\"] = (word_aligned_df[\"onset\"] * target_sample_rate).round() / target_sample_rate\n",
      "/tmp/ipykernel_47749/3278703401.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word_aligned_df[\"offset\"] = (word_aligned_df[\"offset\"] * target_sample_rate).round() / target_sample_rate\n"
     ]
    }
   ],
   "source": [
    "word_aligned_df[\"onset\"] = (word_aligned_df[\"onset\"] * target_sample_rate).round() / target_sample_rate\n",
    "word_aligned_df[\"offset\"] = (word_aligned_df[\"offset\"] * target_sample_rate).round() / target_sample_rate\n",
    "phoneme_aligned_df[\"onset\"] = (phoneme_aligned_df[\"onset\"] * target_sample_rate).round() / target_sample_rate\n",
    "phoneme_aligned_df[\"offset\"] = (phoneme_aligned_df[\"offset\"] * target_sample_rate).round() / target_sample_rate\n",
    "phoneme_aligned_df[\"offset_word\"] = (phoneme_aligned_df[\"offset_word\"] * target_sample_rate).round() / target_sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d52f8b54-ac39-45cb-93c6-6c407b1dffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_onset_samples = (word_aligned_df[\"onset\"] * target_sample_rate).astype(int)\n",
    "phoneme_onset_samples = (phoneme_aligned_df[\"onset\"] * target_sample_rate).astype(int)\n",
    "\n",
    "word_onset_features = np.zeros((len(X), 1))\n",
    "phoneme_onset_features = np.zeros((len(X), 1))\n",
    "word_onset_features[word_onset_samples] = 1\n",
    "phoneme_onset_features[phoneme_onset_samples] = 1\n",
    "\n",
    "X = np.concatenate((X, word_onset_features, phoneme_onset_features), axis=1)\n",
    "ts_feature_names += [\"word_onset\", \"phoneme_onset\"]\n",
    "assert X.shape[1] == len(ts_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6a9a04b-760e-4f75-be6c-1bc52a3d6cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No feature all_phons_onset\n"
     ]
    }
   ],
   "source": [
    "# Remove Heilbron onset data.\n",
    "remove_onset_features = [\"all_words_onset\", \"c_words_onset\", \"all_phons_onset\"]\n",
    "for feat in remove_onset_features:\n",
    "    try:\n",
    "        feat_idx = ts_feature_names.index(feat)\n",
    "    except ValueError:\n",
    "        L.warning(f\"No feature {feat}\")\n",
    "    else:\n",
    "        X = np.delete(X, feat_idx, axis=1)\n",
    "        ts_feature_names = ts_feature_names[:feat_idx] + ts_feature_names[feat_idx + 1:]\n",
    "        \n",
    "assert X.shape[1] == len(ts_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a390af-4501-4259-957a-cae23e7c5584",
   "metadata": {},
   "source": [
    "## Produce BerpDataset representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ece3d43a-0108-45c3-a8bb-cff6272e26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now produce onset information from canonical aligned data.\n",
    "# NB this will contain NaNs if there was misalignment above.\n",
    "word_onsets = word_aligned_df.groupby(\"word_idx\").onset.min().to_dict()\n",
    "word_onsets = torch.tensor([word_onsets.get(word_id.item(), np.nan)\n",
    "                            for word_id in story_stim.word_ids])\n",
    "\n",
    "word_offsets = word_aligned_df.groupby(\"word_idx\").offset.max().to_dict()\n",
    "word_offsets = torch.tensor([word_offsets.get(word_id.item(), np.nan)\n",
    "                             for word_id in story_stim.word_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "788d22b9-a3df-41b3-b46f-4b025f4ebc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute phoneme onsets relative to word onset.\n",
    "phoneme_onsets = phoneme_aligned_df.groupby(\"word_idx\") \\\n",
    "    .apply(lambda xs: list(xs.onset - xs.onset.min())).to_dict()\n",
    "phoneme_onsets = [torch.tensor(phoneme_onsets.get(word_id.item(), [np.nan]))\n",
    "                  for word_id in story_stim.word_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b9f95a4-461a-41c8-b905-2d85627f0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_phonemes = max(len(onsets) for onsets in phoneme_onsets)\n",
    "# Sanity check: max_num_phonemes as computed from aligned data should\n",
    "# match that produced earlier by the natural language stimulus processor\n",
    "assert max_num_phonemes == story_stim.max_n_phonemes, \\\n",
    "    \"%d %d\" % (max_num_phonemes, story_stim.max_n_phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bacdcad4-7106-495d-a484-37b6633a33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad phoneme onset information\n",
    "phoneme_onsets = torch.stack([\n",
    "    pad(onsets, (0, max_num_phonemes - len(onsets)), value=0.)\n",
    "    if len(onsets) < max_num_phonemes\n",
    "    else onsets\n",
    "    for onsets in phoneme_onsets\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75c95267-5d48-4270-ac19-59c8876ae342",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BerpDataset(\n",
    "    name=f\"{story_name}/sub{subject}/run{run}\",\n",
    "    stimulus_name=story_stim.name,\n",
    "    sample_rate=sfreq,\n",
    "    \n",
    "    phonemes=story_stim.phonemes,\n",
    "    \n",
    "    word_onsets=word_onsets,\n",
    "    word_offsets=word_offsets,\n",
    "    phoneme_onsets=phoneme_onsets,\n",
    "    \n",
    "    X_ts=X,\n",
    "    ts_feature_names=ts_feature_names,\n",
    "    \n",
    "    X_variable=X_variable,\n",
    "    variable_feature_names=variable_feature_names,\n",
    "    \n",
    "    Y=y,\n",
    "    sensor_names=sensor_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bfc08d54-edaa-4e43-9178-41b0251a2b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(output_path).open(\"wb\") as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
