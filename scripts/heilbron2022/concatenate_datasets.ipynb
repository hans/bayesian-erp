{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65cf39b3-e270-4873-842b-72014c055482",
   "metadata": {},
   "source": [
    "Concatenate a sequence of `BerpDataset`s and their corresponding `NaturalLanguageStimulus`es into a single `BerpDataset`.\n",
    "\n",
    "Why do this? So that downstream consumers (e.g. the TRF pipeline) see the dataset as one single time series, and don't\n",
    "unnecessarily chop it up and create many invalid boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4860f35-62ce-427a-9082-73a4abb0dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6270c4b-6683-4004-ac6b-4c55415fe50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "742f9484-88db-4534-9629-b69cc6c7740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../..\")\n",
    "from berp.datasets import BerpDataset, NaturalLanguageStimulus, Vocabulary\n",
    "from berp.datasets.base import assert_compatible\n",
    "from berp.datasets.eeg import load_eeg_dataset\n",
    "from berp.util import sample_to_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb3d2eb-35ef-4509-afc3-a85c20f0683d",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    \"../../workflow/heilbron2022/data/dataset/distilgpt2/old-man-and-the-sea/sub17/run1.pkl\",\n",
    "    \"../../workflow/heilbron2022/data/dataset/distilgpt2/old-man-and-the-sea/sub17/run2.pkl\",\n",
    "    \"../../workflow/heilbron2022/data/dataset/distilgpt2/old-man-and-the-sea/sub17/run3.pkl\",\n",
    "]\n",
    "\n",
    "stimuli = {\n",
    "    \"old-man-and-the-sea/run1\": \"../../workflow/heilbron2022/data/stimulus/distilgpt2/old-man-and-the-sea/run1.pkl\",\n",
    "    \"old-man-and-the-sea/run2\": \"../../workflow/heilbron2022/data/stimulus/distilgpt2/old-man-and-the-sea/run2.pkl\",\n",
    "    \"old-man-and-the-sea/run3\": \"../../workflow/heilbron2022/data/stimulus/distilgpt2/old-man-and-the-sea/run3.pkl\",\n",
    "}\n",
    "\n",
    "target_dataset_name = \"old-man-and-the-sea/sub17\"\n",
    "target_stimulus_name = \"old-man-and-the-sea\"\n",
    "\n",
    "out_dataset = \"merged.old-man-and-the-sea.sub17.pkl\"\n",
    "out_stimulus = \"merged.old-man-and-the-sea.pkl\"\n",
    "# If True, pickle stimulus representation\n",
    "save_stimulus = True\n",
    "# If True load stimulus at given out path and check equality\n",
    "check_stimulus = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e850a246-a206-47a8-8285-88b485c4e48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143541/1798871087.py:3: UserWarning: Both save_stimulus and check_stimulus are enabled. Are you sure?\n",
      "  warnings.warn(\"Both save_stimulus and check_stimulus are enabled. Are you sure?\")\n"
     ]
    }
   ],
   "source": [
    "if save_stimulus and check_stimulus:\n",
    "    import warnings\n",
    "    warnings.warn(\"Both save_stimulus and check_stimulus are enabled. Are you sure?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2dc53c-7f18-4582-a497-1873f9608925",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdcb96d1-c165-4702-a154-e3799c9a8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_datasets = load_eeg_dataset(\n",
    "    datasets,\n",
    "    # NB don't do any normalization!\n",
    "    normalize_X_ts=False, normalize_X_variable=False, normalize_Y=False,\n",
    "    stimulus_paths=stimuli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67564681-64fd-40d4-bc25-5e53f35c870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(nested_datasets.datasets) == len(datasets)\n",
    "assert all(ds.global_slice_indices == None for ds in nested_datasets.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b89bdbc9-5347-43a5-96d8-15b73ea028f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = nested_datasets.datasets[0]\n",
    "for ds2 in nested_datasets.datasets[1:]:\n",
    "    assert_compatible(ds1, ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f4feaf7-dc3b-4e4c-92c1-4d99769fc536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.      , 170.359375, 334.09375 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute onset of each dataset in the eventual merged data.\n",
    "dataset_durations = [sample_to_time(len(dataset) + 1, dataset.sample_rate)\n",
    "                     for dataset in nested_datasets.datasets]\n",
    "dataset_onsets = np.concatenate([[0], np.cumsum(dataset_durations[:-1])])\n",
    "dataset_onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14305b1f-43aa-4fd7-9810-409b84861a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds = nested_datasets.datasets\n",
    "ds1 = all_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821537d-6582-45b3-a94e-f0eb5c5afd6e",
   "metadata": {},
   "source": [
    "## Load stimulus representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c27cfd85-5b5d-4d9e-9514-21e21122dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stims = {}\n",
    "for name, path in stimuli.items():\n",
    "    with open(path, \"rb\") as f:\n",
    "        stims[name] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "755b962f-c2f4-4046-8cd8-b645750bd598",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_order = [ds.stimulus_name for ds in all_ds]\n",
    "stims_list = [stims[name] for name in stim_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37528f62-5b25-4e32-a46d-13edfb41984d",
   "metadata": {},
   "source": [
    "## Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32c9756d-a782-4287-951c-d52e21768421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad phoneme_onsets as necessary\n",
    "max_num_phonemes = max(ds.max_n_phonemes for ds in all_ds)\n",
    "assert max_num_phonemes == max(stim.max_n_phonemes for stim in stims_list)\n",
    "\n",
    "# NB we don't apply dataset-level onset here, since phoneme_onsets are all relative\n",
    "# to word onset\n",
    "merged_phoneme_onsets = [\n",
    "    pad(ds.phoneme_onsets, (0, max_num_phonemes - ds.phoneme_onsets.shape[1]), value=0)\n",
    "    if ds.phoneme_onsets.shape[1] < max_num_phonemes\n",
    "    else ds.phoneme_onsets\n",
    "    for ds in all_ds\n",
    "]\n",
    "merged_phoneme_onsets = torch.cat(merged_phoneme_onsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeab55e2-c7ed-4bb1-bb1f-4727a5d191d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = BerpDataset(\n",
    "    name=target_dataset_name, stimulus_name=target_stimulus_name,\n",
    "    sample_rate=ds1.sample_rate,\n",
    "    \n",
    "    # Shift word onset and offset information based on dataset onset\n",
    "    word_onsets=torch.cat([ds.word_onsets + dataset_onset for ds, dataset_onset in zip(all_ds, dataset_onsets)]),\n",
    "    word_offsets=torch.cat([ds.word_offsets + dataset_onset for ds, dataset_onset in zip(all_ds, dataset_onsets)]),\n",
    "    # NB we don't apply dataset-level onset here, since phoneme_onsets are all relative\n",
    "    # to word onset\n",
    "    phoneme_onsets=merged_phoneme_onsets,\n",
    "    \n",
    "    X_ts=torch.cat([ds.X_ts for ds in all_ds]),\n",
    "    X_variable=torch.cat([ds.X_variable for ds in all_ds]),\n",
    "    Y=torch.cat([ds.Y for ds in all_ds]),\n",
    "    \n",
    "    sensor_names=ds1.sensor_names,\n",
    "    phonemes=ds1.phonemes,\n",
    "    \n",
    "    ts_feature_names=ds1.ts_feature_names,\n",
    "    variable_feature_names=ds1.variable_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62bef98-d313-4cbb-9f1c-f58de4e12589",
   "metadata": {},
   "source": [
    "## Merge stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24d251ab-5ed3-4744-aebd-82a8f6d75374",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim1 = stims_list[0]\n",
    "for stim2 in stims_list[1:]:\n",
    "    assert stim1.phonemes == stim2.phonemes\n",
    "    assert stim1.pad_phoneme_id == stim2.pad_phoneme_id\n",
    "    assert stim1.word_features.shape[1] == stim2.word_features.shape[1]\n",
    "    # candidate size should match\n",
    "    assert stim1.p_candidates.shape[1] == stim2.p_candidates.shape[1]\n",
    "    assert stim1.candidate_ids.shape[1] == stim2.candidate_ids.shape[1]\n",
    "    \n",
    "    assert stim1.word_feature_names == stim2.word_feature_names\n",
    "    assert stim1.phoneme_feature_names == stim2.phoneme_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faed8757-8859-440c-b0c8-bb38ef2262fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id_offsets = np.concatenate([[0], np.cumsum([stim.word_ids.max().item() + 1 for stim in stims_list])[:-1]])\n",
    "merged_word_ids = torch.cat([stim.word_ids + word_id_offsets[i] for i, stim in enumerate(stims_list)])\n",
    "\n",
    "# Merged word IDs should be unique.\n",
    "assert (torch.bincount(merged_word_ids) <= 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c8270fe-66fe-4ee5-b238-038f10cc163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of stim1 vocabulary: 17099\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34ec2e21e4d425eaaa60ff491936d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of merged vocabulary: 19520\n"
     ]
    }
   ],
   "source": [
    "# Merge vocabularies.\n",
    "merged_vocabulary = deepcopy(stim1.candidate_vocabulary)\n",
    "print(f\"Size of stim1 vocabulary: {len(merged_vocabulary)}\")\n",
    "\n",
    "for stim2 in tqdm(stims_list[1:]):\n",
    "    for vocab2_idx, vocab2_tok in enumerate(stim2.candidate_vocabulary.idx2tok):\n",
    "        vocab1_idx = merged_vocabulary.add(vocab2_tok)\n",
    "        stim2.candidate_ids[stim2.candidate_ids == vocab2_idx] = vocab1_idx\n",
    "        \n",
    "print(f\"Size of merged vocabulary: {len(merged_vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2fa3b01-37c7-4f73-b0e0-b41b503e45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stimulus = NaturalLanguageStimulus(\n",
    "    name=target_stimulus_name,\n",
    "    phonemes=stim1.phonemes,\n",
    "    pad_phoneme_id=stim1.pad_phoneme_id,\n",
    "    \n",
    "    word_ids=merged_word_ids,\n",
    "    word_lengths=torch.cat([stim.word_lengths for stim in stims_list]),\n",
    "    word_features=torch.cat([stim.word_features for stim in stims_list]),\n",
    "    word_feature_names=stim1.word_feature_names,\n",
    "    \n",
    "    phoneme_features=list(itertools.chain.from_iterable(stim.phoneme_features for stim in stims_list)),\n",
    "    phoneme_feature_names=stim1.phoneme_feature_names,\n",
    "    \n",
    "    p_candidates=torch.cat([stim.p_candidates for stim in stims_list]),\n",
    "    candidate_ids=torch.cat([stim.candidate_ids for stim in stims_list]),\n",
    "    candidate_vocabulary=merged_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85766d4-3e5f-4b5b-8730-24f70a242843",
   "metadata": {},
   "source": [
    "## Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e19934af-9fdd-43bb-a01e-2c61f46685dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_dataset, \"wb\") as f:\n",
    "    pickle.dump(merged_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8137231-3c41-41ae-8e77-54be47eed99a",
   "metadata": {},
   "source": [
    "## Check stimulus equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd20ee1b-7290-42de-93e7-a64916615701",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_stimulus, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     ref_stimulus \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ref_stimulus \u001b[38;5;241m==\u001b[39m merged_stimulus\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if check_stimulus:\n",
    "    with open(out_stimulus, \"rb\") as f:\n",
    "        ref_stimulus = pickle.load(f)\n",
    "    assert ref_stimulus == merged_stimulus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6642e51f-ab6c-403e-ba0d-29d2d7c99e54",
   "metadata": {},
   "source": [
    "## Save stimulus, possibly overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f3299-7618-4525-b5d6-ab455d0bad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_stimulus:\n",
    "    with open(out_stimulus, \"wb\") as f:\n",
    "        pickle.dump(merged_stimulus, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
